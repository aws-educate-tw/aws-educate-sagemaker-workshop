{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_model(initial_messages):\n",
    "    history = initial_messages\n",
    "    response_complete = False\n",
    "    while not response_complete:\n",
    "        stream = ollama.chat(\n",
    "            model=\"model-name\",\n",
    "            messages=history,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "\n",
    "        for chunk in stream:\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                response_part = chunk['message']['content']\n",
    "                full_response += response_part\n",
    "                sys.stdout.write(response_part)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                match = re.search(r'```Python-exe\\n(.*?)```',\n",
    "                                  full_response, re.DOTALL)\n",
    "                if match:\n",
    "                    code = match.group(1)\n",
    "                    execution_result = execute_Python_code(code.strip())\n",
    "                    print(f\"\\nExecuted Result: {execution_result}\")\n",
    "\n",
    "                    if execution_result.strip():\n",
    "                        history.append(\n",
    "                            {\"role\": \"assistant\", \"content\": full_response})\n",
    "                        history.append(\n",
    "                            {'role': 'user', 'content': \"Executed Result: \" + execution_result.strip()})\n",
    "                    else:\n",
    "                        history.append(\n",
    "                            {\"role\": \"assistant\", \"content\": full_response})\n",
    "                        history.append({\"role\": \"user\", \"content\": full_response +\n",
    "                                       f\"\\nExecution {tool_type} is successful without outputs\"})\n",
    "                    break\n",
    "\n",
    "        # If code was executed, we will contiune the loop and feed the model with executed outputs\n",
    "        if match:\n",
    "            continue\n",
    "        else:\n",
    "            print()  # Move to the next line if no code was detected and streaming finished\n",
    "            history.append({'role': 'assistant', 'content': full_response})\n",
    "            # Exit the while loop as normal continuation if no code block found\n",
    "            response_complete = True\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'AWSenv (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n AWSenv ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    system_message = \"\"\"system message defined above\"\"\"\n",
    "    history = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    }]\n",
    "    print(\"OLLAMA Chat Interface. Press CTRL+C to interrupt the response or CTRL+D to exit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\">>> \")\n",
    "            if user_input.lower() == \"/exit\":\n",
    "                print(\"Exiting chat.\")\n",
    "                break\n",
    "\n",
    "            history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            \n",
    "            try:\n",
    "                # Process interaction with model including execution of code blocks\n",
    "                history = interact_with_model(history)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nResponse interrupted by user.\")\n",
    "                history.append({'role': 'assistant', 'content': '[Interrupted by user]'})\n",
    "                print()  # Ensure the next user prompt appears on a new line\n",
    "\n",
    "    except EOFError:\n",
    "        print(\"\\nChat terminated via CTRL+D.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWSenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
