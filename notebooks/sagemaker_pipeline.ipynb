{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff841a2-306b-4a09-a248-d6dc01b94ec5",
   "metadata": {},
   "source": [
    "_API Reference: https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#steps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4581744-4b63-4dc9-b506-4c6ca9e6c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker transformers==4.44.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4382b5-28fb-4434-a3c1-88bd6ad252b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import TrainingStep, ProcessingStep, CreateModelStep, CacheConfig\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e7ada8-41b9-4bfd-a5c7-0352a433bb2c",
   "metadata": {},
   "source": [
    "## 取得 Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b21e08c-0e15-4188-84e3-17e5e85484fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# 初始化 S3 客戶端，來源區域是 ap-northeast-1，目標區域是 us-west-2\n",
    "s3_source = boto3.client('s3', region_name=\"us-west-2\")\n",
    "s3_target = boto3.client('s3', region_name=\"us-west-2\")\n",
    "\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def copy_s3_object(source_uri, target_bucket):\n",
    "    source_bucket, source_key = parse_s3_uri(source_uri)\n",
    "    try:\n",
    "        # 從來源 bucket 下載檔案\n",
    "        response = s3_source.get_object(Bucket=source_bucket, Key=source_key)\n",
    "        file_content = response['Body'].read()\n",
    "\n",
    "        # 將檔案上傳到目標 bucket\n",
    "        s3_target.put_object(Bucket=target_bucket, Key=source_key, Body=file_content)\n",
    "        print(f\"Copied {source_key} to {target_bucket}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {source_key}: {str(e)}\")\n",
    "\n",
    "# s3 URI\n",
    "base_uri = 's3://aws-educate-09-28-sagemaker-workshop-oregon/datasets/phi-3/'\n",
    "train_uri = base_uri + 'train_dataset.json'\n",
    "test_uri = base_uri + 'test_dataset.json'\n",
    "\n",
    "# 你的目標 S3 bucket\n",
    "target_bucket = sess.default_bucket()\n",
    "\n",
    "\n",
    "# 複製 train 和 test 資料到新的 S3 bucket\n",
    "copy_s3_object(train_uri, target_bucket)\n",
    "copy_s3_object(test_uri, target_bucket)\n",
    "\n",
    "datasets_s3_uri = \"s3://\" + target_bucket + \"/datasets/phi-3/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828512d-0375-4104-b71c-9769f7f7b0a8",
   "metadata": {},
   "source": [
    "## Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3fe262-89b9-4c0e-90a8-596cd554eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義參數\n",
    "training_datasets_s3_uri = ParameterString(name=\"TrainingDatasetesS3Uri\", default_value=datasets_s3_uri)\n",
    "inference_instance_type = ParameterString(name=\"InferenceInstanceType\", default_value=\"ml.g5.xlarge\")\n",
    "\n",
    "\n",
    "# 其他配置\n",
    "model_package_group_name = \"Demo-SageMaker-Pipeline-Group\"\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcaf85-e1ad-42a5-a4b8-2e337b9a30b6",
   "metadata": {},
   "source": [
    "## TraningStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebfaa7a-5c79-47ae-99d9-05ce290e85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 3,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 2,                 # Number of updates steps to accumulate \n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'fp16': True ,\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"constant\",                   # learning rate scheduler\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run',                         # output directory, where to save assets during training\n",
    "}\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}'\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora.py',    # train script\n",
    "    source_dir           = '../scripts',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")\n",
    "\n",
    "\n",
    "# 定義訓練步驟\n",
    "train_step = TrainingStep(\n",
    "    name=\"TrainModel\",\n",
    "    estimator=huggingface_estimator,\n",
    "    inputs={\n",
    "        'training': TrainingInput(training_datasets_s3_uri, content_type=\"application/json\")\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b536da2-8cfe-4f15-a132-e71fcba80a04",
   "metadata": {},
   "source": [
    "## Register Model Step\n",
    "Documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-register-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acd3e52-9dac-4f51-885a-bf8dab1bd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "    'SM_NUM_GPUS': json.dumps(1), # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel( # https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts, # https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html\n",
    "    role=role,\n",
    "    image_uri=llm_image,\n",
    "    sagemaker_session=sess,\n",
    "    env=config\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[inference_instance_type],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829532f-15ed-42dc-853c-e4e7c9f4d2ac",
   "metadata": {},
   "source": [
    "## Lambda Step for Deploying model endpoint\n",
    "documents: \n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-lambda\n",
    "- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f65fb8e-a048-4019-8e30-e0475271ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_deployer.py\n",
    "\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "\n",
    "import time\n",
    "\n",
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%H-%M\", time.localtime())\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Lambda function to deploy a model to an Endpoint using boto3 and create an API Gateway for SageMaker\"\"\"\n",
    "\n",
    "    # 使用 event 中的參數\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    model_name = event[\"model_name\"] + \"-run-at-\" + current_time\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "    apigateway_role = event[\"apigateway_role\"]\n",
    "    inference_instance_type = event[\"inference_instance_type\"]\n",
    "\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "    apigateway_client = boto3.client(\"apigateway\")\n",
    "\n",
    "    # 創建 SageMaker 模型\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\"ModelPackageName\": model_package_arn},\n",
    "        ExecutionRoleArn=role,\n",
    "    )\n",
    "\n",
    "    # 創建端點配置\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": inference_instance_type,\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 創建端點\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "    # 創建 API Gateway\n",
    "    api_name = f\"{model_name}-api\"\n",
    "    create_api_response = apigateway_client.create_rest_api(\n",
    "        name=api_name,\n",
    "        description=f\"API Gateway for SageMaker endpoint {endpoint_name}\",\n",
    "        endpointConfiguration={\"types\": [\"REGIONAL\"]},\n",
    "    )\n",
    "\n",
    "    # 取得 API Gateway 的根資源 ID\n",
    "    api_id = create_api_response[\"id\"]\n",
    "    root_id = apigateway_client.get_resources(restApiId=api_id)[\"items\"][0][\"id\"]\n",
    "\n",
    "    # 創建 POST 方法並設定\n",
    "    apigateway_client.put_method(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        authorizationType=\"NONE\",\n",
    "    )\n",
    "\n",
    "    # 設置 SageMaker Runtime 與 API Gateway 的整合\n",
    "    apigateway_client.put_integration(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        type=\"AWS\",  # 使用 AWS Service Integration\n",
    "        integrationHttpMethod=\"POST\",\n",
    "        uri=f\"arn:aws:apigateway:{boto3.Session().region_name}:runtime.sagemaker:path/endpoints/{endpoint_name}/invocations\",\n",
    "        credentials=apigateway_role,  # 指定具有 SageMaker InvokeEndpoint 權限的角色\n",
    "    )\n",
    "\n",
    "    # 設置 Integration Response，以便 API Gateway 正確處理 SageMaker 回應\n",
    "    apigateway_client.put_integration_response(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        statusCode=\"200\",\n",
    "        responseTemplates={\"application/json\": \"$input.body\"},\n",
    "    )\n",
    "\n",
    "    # 創建方法回應，確保有適當的狀態碼\n",
    "    apigateway_client.put_method_response(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        statusCode=\"200\",\n",
    "        responseModels={\"application/json\": \"Empty\"},\n",
    "    )\n",
    "\n",
    "    # 部署 API Gateway\n",
    "    apigateway_client.create_deployment(\n",
    "        restApiId=api_id,\n",
    "        stageName=\"dev\",\n",
    "    )\n",
    "\n",
    "    # 返回 API Gateway 的 URL\n",
    "    api_url = (\n",
    "        f\"https://{api_id}.execute-api.{boto3.Session().region_name}.amazonaws.com/dev\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\n",
    "            {\n",
    "                \"message\": f\"Created Endpoint {endpoint_name} and API Gateway {api_name}!\",\n",
    "                \"endpoint_name\": endpoint_name,\n",
    "                \"api_url\": api_url,\n",
    "            }\n",
    "        ),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bbe4780-fe3e-4279-9d10-a64cd1d56756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "def attach_policy_if_missing(role_name, policy_arn):\n",
    "    \"\"\"檢查並附加策略到角色，若策略已存在則跳過\"\"\"\n",
    "    try:\n",
    "        # 獲取附加到角色的現有策略\n",
    "        attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        attached_policy_arns = [policy['PolicyArn'] for policy in attached_policies]\n",
    "\n",
    "        # 如果策略不在已附加列表中，則附加\n",
    "        if policy_arn not in attached_policy_arns:\n",
    "            iam.attach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "            print(f\"Attached policy {policy_arn} to role {role_name}\")\n",
    "        else:\n",
    "            print(f\"Policy {policy_arn} already attached to role {role_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy {policy_arn} to role {role_name}: {e}\")\n",
    "\n",
    "def create_lambda_role(role_name, apigateway_role_arn):\n",
    "    try:\n",
    "        # 創建 IAM 角色\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to interact with SageMaker, API Gateway, and Bedrock'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    # 無論角色是否已存在，都會檢查並附加所需的策略\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole')\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess')\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/AmazonAPIGatewayAdministrator')\n",
    "\n",
    "    # 附加 Lambda Function URL 創建的策略\n",
    "    policy_document_url_config = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \"lambda:CreateFunctionUrlConfig\",\n",
    "                \"Resource\": f\"arn:aws:lambda:*:{role_arn.split(':')[4]}:function:*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=\"LambdaURLConfigPolicy\",\n",
    "        PolicyDocument=json.dumps(policy_document_url_config)\n",
    "    )\n",
    "\n",
    "    # 在 Lambda 角色中附加 iam:PassRole 權限，允許傳遞 API Gateway 的角色\n",
    "    policy_document_pass_role = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \"iam:PassRole\",\n",
    "                \"Resource\": apigateway_role_arn\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=\"PassRolePolicy\",\n",
    "        PolicyDocument=json.dumps(policy_document_pass_role)\n",
    "    )\n",
    "\n",
    "    # 附加 Bedrock 權限\n",
    "    bedrock_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                    \"bedrock:ListModels\",\n",
    "                    \"bedrock:InvokeModelWithResponseStream\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=\"BedrockInvokePolicy\",\n",
    "        PolicyDocument=json.dumps(bedrock_policy_document)\n",
    "    )\n",
    "\n",
    "    print(f\"Attached iam:PassRole, lambda:CreateFunctionUrlConfig, and Bedrock policies to {role_name}\")\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_apigateway_role(role_name):\n",
    "    try:\n",
    "        # 創建 API Gateway 用的 IAM 角色\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"apigateway.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for API Gateway to invoke SageMaker Endpoints'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    # 附加允許 API Gateway 調用 SageMaker 的策略\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess')\n",
    "\n",
    "    return role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b202ea-386c-42c4-b9fd-cb3763b36ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "apigateway_role = create_apigateway_role(\"apigateway-role\")\n",
    "lambda_role = create_lambda_role(\"lambda-deployment-role\", apigateway_role_arn=apigateway_role)\n",
    "\n",
    "\n",
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "model_name = \"demo-sagemaker-pipeline-model\" + current_time\n",
    "endpoint_name = \"demo-sagemaker-pipeline-endpoint-\" + current_time\n",
    "endpoint_config_name = \"demo-sagemaker-pipeline-endpoint-config\" + current_time\n",
    "function_name = \"demo-sagemaker-pipeline-lambda-step\" + current_time\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function\n",
    "# in the Lambda\n",
    "lambda_deploy_step = LambdaStep(\n",
    "    name=\"LambdaStepDeployModelEndpoint\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_package_arn\": register_step.properties.ModelPackageArn,\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"role\": role,\n",
    "        \"apigateway_role\": apigateway_role,\n",
    "        \"inference_instance_type\": inference_instance_type\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0ac9e-9476-48db-ba5c-53a0cf6279d9",
   "metadata": {},
   "source": [
    "## CreateStreamingResponseLambdaFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "914fe446-a8c4-43d1-89e0-0a119fd83e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import boto3\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import RedirectResponse, StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-runtime\")\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "SAGEMAKER_ENDPOINT_NAME = os.getenv(\"SAGEMAKER_ENDPOINT_NAME\")\n",
    "\n",
    "# CORS 設定\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "app.mount(\"/demo\", StaticFiles(directory=\"static\", html=True))\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return RedirectResponse(url=\"/demo/\")\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str  # Role can be 'user' or 'assistant'\n",
    "    content: str  # The content of the message\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    model: str  # Model name provided by the client\n",
    "    system: Optional[str] = None  # Optional system prompt\n",
    "    messages: List[Message]  # List of messages with roles and content\n",
    "    temperature: Optional[float] = 0.5  # Optional, default temperature is 0.5\n",
    "    max_tokens: Optional[int] = 1024  # Optional, default max_tokens is 1024\n",
    "    stream: Optional[bool] = True  # Enable streaming by default\n",
    "\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "def api_chat_completion(chat_request: ChatRequest):\n",
    "    if not chat_request.messages:\n",
    "        return {\"error\": \"Messages are required\"}\n",
    "\n",
    "    # 如果 model 是 'psy-1'，調用 SageMaker 端點\n",
    "    if chat_request.model == \"psy-1\":\n",
    "        body = {\n",
    "            \"inputs\": f\"<|system|>{chat_request.system or ''}<|user|>{chat_request.messages[-1].content}<|assistant|>\",\n",
    "            \"parameters\": {\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": chat_request.temperature,\n",
    "                \"max_new_tokens\": chat_request.max_tokens,\n",
    "                \"repetition_penalty\": 1.03,\n",
    "                \"stop\": [\"\\nUser:\", \"<|endoftext|>\", \"###\"],\n",
    "            },\n",
    "            \"stream\": chat_request.stream,\n",
    "        }\n",
    "        return StreamingResponse(\n",
    "            sagemaker_stream(\n",
    "                SAGEMAKER_ENDPOINT_NAME, body\n",
    "            ),\n",
    "            media_type=\"text/html\",\n",
    "        )\n",
    "\n",
    "    # Default to Bedrock model\n",
    "    body = {\n",
    "        \"max_tokens\": chat_request.max_tokens,  # Accept max_tokens from the front-end\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",  # Required by Bedrock API\n",
    "        \"messages\": [\n",
    "            {\"role\": msg.role, \"content\": msg.content} for msg in chat_request.messages\n",
    "        ],\n",
    "        \"temperature\": chat_request.temperature,  # Accept temperature from the front-end\n",
    "    }\n",
    "\n",
    "    # Include the system prompt if provided\n",
    "    if chat_request.system:\n",
    "        body[\"system\"] = chat_request.system\n",
    "\n",
    "    return StreamingResponse(\n",
    "        bedrock_stream(chat_request.model, body), media_type=\"text/html\"\n",
    "    )\n",
    "\n",
    "async def bedrock_stream(model_id: str, body: dict):\n",
    "    # Convert the dictionary into a JSON string\n",
    "    body_str = json.dumps(body)\n",
    "\n",
    "    # Send the model ID from the request and the body to Bedrock\n",
    "    response = bedrock.invoke_model_with_response_stream(\n",
    "        modelId=model_id,  # Model name provided in the request body\n",
    "        body=body_str,\n",
    "    )\n",
    "\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                message = json.loads(chunk.get(\"bytes\").decode())\n",
    "                if message[\"type\"] == \"content_block_delta\":\n",
    "                    # Stream the content back to the client\n",
    "                    yield message[\"delta\"][\"text\"] or \"\"\n",
    "                elif message[\"type\"] == \"message_stop\":\n",
    "                    # Indicate the end of the message\n",
    "                    yield \"\\n\"\n",
    "\n",
    "async def sagemaker_stream(endpoint_name: str, body: dict):\n",
    "    body_str = json.dumps(body)\n",
    "\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=body_str,\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "    stream = response['Body']\n",
    "    complete_text = \"\"  # 用來存儲最終的合併文本\n",
    "    incomplete_message = \"\"  # 用來存儲不完整的消息\n",
    "\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'PayloadPart' in event:\n",
    "                try:\n",
    "                    raw_message = event['PayloadPart']['Bytes']\n",
    "                    # 設定解碼錯誤策略，允許跳過無效字元\n",
    "                    decoded_message = raw_message.decode('utf-8', errors='ignore')\n",
    "                    print(f\"Raw Message after stripping 'data:': {decoded_message}\")\n",
    "\n",
    "                    if decoded_message.startswith(\"data:\"):\n",
    "                        decoded_message = decoded_message[5:].strip()\n",
    "\n",
    "                    # 將不完整的訊息拼接起來\n",
    "                    if incomplete_message:\n",
    "                        decoded_message = incomplete_message + decoded_message\n",
    "                        incomplete_message = \"\"\n",
    "\n",
    "                    # 嘗試解析為 JSON\n",
    "                    try:\n",
    "                        json_message = json.loads(decoded_message)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # 如果 JSON 解析失敗，將訊息暫存並等待下一個 event\n",
    "                        incomplete_message = decoded_message\n",
    "                        continue\n",
    "\n",
    "                    # 確認 token 存在且非特殊字符\n",
    "                    token = json_message.get('token', {}).get('text')\n",
    "                    special = json_message.get('token', {}).get('special', False)\n",
    "\n",
    "                    # 檢查 token 是否是 \"<|end|>\" 或 special 為 true，直接終止流\n",
    "                    if token == \"<|end|>\" or special:\n",
    "                        print(\"End token detected, stopping stream.\")\n",
    "                        break  # 結束迴圈並停止拼接\n",
    "\n",
    "                    if token and token.strip():\n",
    "                        complete_text += token  # 拼接完整的 text\n",
    "                        yield token  # 實時回傳 token\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"JSON Decode Error: Skipping invalid JSON data.\")\n",
    "                    continue\n",
    "\n",
    "            elif 'ModelStreamError' in event:\n",
    "                error_message = event['ModelStreamError']['Message']\n",
    "                print(f\"Model stream error: {error_message}\")\n",
    "                # 過濾錯誤訊息，不傳回給前端\n",
    "                continue\n",
    "\n",
    "            elif 'InternalStreamFailure' in event:\n",
    "                print(f\"Internal stream failure: {event['InternalStreamFailure']['Message']}\")\n",
    "                # 過濾錯誤訊息，不傳回給前端\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", \"8080\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c5cf242-16b4-4842-9257-45603a01fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "PATH=$PATH:$LAMBDA_TASK_ROOT/bin \\\n",
    "    PYTHONPATH=$PYTHONPATH:/opt/python:$LAMBDA_RUNTIME_DIR \\\n",
    "    exec python -m uvicorn --port=$PORT main:app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4517cd46-7e37-468c-8307-207578d7c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1593f9e8-a562-44ee-a9a5-822bd29474b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile static/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Psy test demo</title>\n",
    "    <link rel=\"stylesheet\" href=\"style.css\" />\n",
    "    <link\n",
    "      rel=\"stylesheet\"\n",
    "      href=\"https://fonts.googleapis.com/css?family=Roboto:300,300italic,700,700italic\"\n",
    "    />\n",
    "    <link\n",
    "      rel=\"stylesheet\"\n",
    "      href=\"https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.css\"\n",
    "    />\n",
    "    <link\n",
    "      rel=\"stylesheet\"\n",
    "      href=\"https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.1/milligram.css\"\n",
    "    />\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <div id=\"container\" class=\"row\">\n",
    "      <div class=\"column column-67\">\n",
    "        <h1>Psy test demo</h1>\n",
    "        <h4>\n",
    "          Enter your request details below and let the AI generate a response.\n",
    "        </h4>\n",
    "\n",
    "        <!-- Input fields for model, system prompt, and user message -->\n",
    "        <label for=\"model\">Model:</label>\n",
    "        <small\n",
    "          ><a\n",
    "            href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html\"\n",
    "            target=\"_blank\"\n",
    "            >Refer to the AWS Bedrock model IDs documentation</a\n",
    "          ></small\n",
    "        >\n",
    "        <input\n",
    "          type=\"text\"\n",
    "          id=\"model\"\n",
    "          placeholder=\"Enter model ID (e.g., 'anthropic.claude-3-sonnet-20240229-v1:0')\"\n",
    "          value=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "        />\n",
    "\n",
    "        <label for=\"system\">System Prompt:</label>\n",
    "        <textarea id=\"system\" placeholder=\"Enter system prompt\"></textarea>\n",
    "\n",
    "        <label for=\"user-message\">User Message:</label>\n",
    "        <textarea id=\"user-message\" placeholder=\"Enter user message\"></textarea>\n",
    "\n",
    "        <!-- Input for max_tokens and temperature -->\n",
    "        <label for=\"max-tokens\">Max Tokens:</label>\n",
    "        <input\n",
    "          type=\"number\"\n",
    "          id=\"max-tokens\"\n",
    "          value=\"1024\"\n",
    "          min=\"1\"\n",
    "          placeholder=\"Enter max tokens\"\n",
    "        />\n",
    "\n",
    "        <label for=\"temperature\">Temperature:</label>\n",
    "        <input\n",
    "          type=\"number\"\n",
    "          step=\"0.1\"\n",
    "          id=\"temperature\"\n",
    "          value=\"0.5\"\n",
    "          min=\"0\"\n",
    "          max=\"1\"\n",
    "          placeholder=\"Enter temperature\"\n",
    "        />\n",
    "\n",
    "        <!-- Button to trigger the API call -->\n",
    "        <button id=\"generate-response\">Generate</button>\n",
    "\n",
    "        <!-- Output area for the story -->\n",
    "        <div id=\"story-output\"></div>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "    <script src=\"script.js\"></script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2133cb4-ad2f-4406-ab90-933a766ee9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile static/script.js\n",
    "\n",
    "async function generateAIResponse() {\n",
    "  // Get input values from the form\n",
    "  const model = document.getElementById(\"model\").value;\n",
    "  const system = document.getElementById(\"system\").value;\n",
    "  const userMessage = document.getElementById(\"user-message\").value;\n",
    "  const maxTokens = document.getElementById(\"max-tokens\").value;\n",
    "  const temperature = document.getElementById(\"temperature\").value;\n",
    "\n",
    "  if (userMessage.trim().length === 0) {\n",
    "    return;\n",
    "  }\n",
    "\n",
    "  const storyOutput = document.getElementById(\"story-output\");\n",
    "  storyOutput.innerText = \"Thinking...\";\n",
    "\n",
    "  try {\n",
    "    // Create request payload\n",
    "    const requestBody = {\n",
    "      model: model,\n",
    "      system: system,\n",
    "      messages: [{\n",
    "        role: 'user',\n",
    "        content: userMessage\n",
    "      }],\n",
    "      max_tokens: parseInt(maxTokens),\n",
    "      temperature: parseFloat(temperature),\n",
    "      stream: true\n",
    "    };\n",
    "\n",
    "    // Use Fetch API to send a POST request for response streaming\n",
    "    const response = await fetch(\"/v1/chat/completions\", {\n",
    "      method: \"POST\",\n",
    "      headers: {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "      },\n",
    "      body: JSON.stringify(requestBody)\n",
    "    });\n",
    "\n",
    "    storyOutput.innerText = \"\";\n",
    "\n",
    "    // Response Body is a ReadableStream\n",
    "    const reader = response.body.getReader();\n",
    "    const decoder = new TextDecoder();\n",
    "\n",
    "    // Process the chunks from the stream\n",
    "    while (true) {\n",
    "      const {\n",
    "        done,\n",
    "        value\n",
    "      } = await reader.read();\n",
    "      if (done) {\n",
    "        break;\n",
    "      }\n",
    "      const text = decoder.decode(value);\n",
    "      storyOutput.innerText += text;\n",
    "    }\n",
    "\n",
    "  } catch (error) {\n",
    "    storyOutput.innerText = `Sorry, an error happened. Please try again later. \\n\\n ${error}`;\n",
    "  }\n",
    "}\n",
    "\n",
    "document.getElementById(\"generate-response\").addEventListener(\"click\", generateAIResponse);\n",
    "document.getElementById('user-message').addEventListener('keydown', function (e) {\n",
    "  if (e.code === 'Enter') {\n",
    "    generateAIResponse();\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c2ff63-d641-4912-91fb-d7173d0c934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile static/style.css\n",
    "\n",
    "body {\n",
    "    font-family: sans-serif;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "  }\n",
    "  \n",
    "  #container {\n",
    "    justify-content: center\n",
    "  }\n",
    "  \n",
    "  h1 {\n",
    "    text-align: center;\n",
    "  }\n",
    "  \n",
    "  p {\n",
    "    margin-bottom: 10px;\n",
    "  }\n",
    "  \n",
    "  input {\n",
    "    width: 100%;\n",
    "    height: 20px;\n",
    "    border: 1px solid black;\n",
    "    margin-bottom: 10px;\n",
    "  }\n",
    "  \n",
    "  button {\n",
    "    height: 20px;\n",
    "    background-color: #000;\n",
    "    color: #fff;\n",
    "    border: none;\n",
    "    cursor: pointer;\n",
    "  }\n",
    "  \n",
    "  #story-output {\n",
    "    width: 100%;\n",
    "    overflow: auto;\n",
    "  }\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9b6b15-6945-487b-b297-6a7c2c5a4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Define the names of your files and directories\n",
    "main_script = 'main.py'\n",
    "run_script = 'run.sh'\n",
    "static_dir = 'static'\n",
    "zip_filename = 'lambda_package.zip'\n",
    "\n",
    "# Step 1: Create a temporary directory to store the files\n",
    "if not os.path.exists('lambda_temp'):\n",
    "    os.mkdir('lambda_temp')\n",
    "\n",
    "# Step 2: Copy your scripts (main.py and run.sh) into the temp directory\n",
    "shutil.copy(main_script, './lambda_temp/')\n",
    "shutil.copy(run_script, './lambda_temp/')\n",
    "\n",
    "# Step 3: Copy the 'static' directory into the temp directory\n",
    "if os.path.exists(static_dir):\n",
    "    shutil.copytree(static_dir, './lambda_temp/static')\n",
    "\n",
    "# Step 4: Create the .zip package\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk('lambda_temp'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, arcname=os.path.relpath(file_path, 'lambda_temp'))\n",
    "\n",
    "# Step 5: Clean up the temp directory\n",
    "shutil.rmtree('lambda_temp')\n",
    "\n",
    "print(f\"{zip_filename} created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2e2e057-c4de-4a81-8368-b7be07a39ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import subprocess\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# 定義變數\n",
    "layer_dir = \"./layer/python\"\n",
    "zip_filename = \"layer.zip\"\n",
    "region = sess.boto_region_name\n",
    "layer_name = \"fast_api_related_lambda_layer\"\n",
    "layer_description = \"Lambda layer for FastAPI with Lambda Web Adapter\"\n",
    "\n",
    "# Step 1: 建立 layer/python 資料夾\n",
    "if not os.path.exists(layer_dir):\n",
    "    os.makedirs(layer_dir)\n",
    "\n",
    "# Step 2: 安裝指定的套件到 layer/python 資料夾\n",
    "subprocess.run([\n",
    "    \"pip3\", \"install\", \"--target\", layer_dir, \n",
    "    \"annotated-types==0.6.0\", \n",
    "    \"anyio==4.2.0\", \n",
    "    \"click==8.1.7\", \n",
    "    \"exceptiongroup==1.2.0\", \n",
    "    \"fastapi==0.109.2\", \n",
    "    \"h11==0.14.0\", \n",
    "    \"idna==3.7\", \n",
    "    \"pydantic==2.6.1\", \n",
    "    \"pydantic_core==2.16.2\", \n",
    "    \"sniffio==1.3.0\", \n",
    "    \"starlette==0.36.3\", \n",
    "    \"typing_extensions==4.9.0\", \n",
    "    \"uvicorn==0.27.0.post1\"\n",
    "], check=True)\n",
    "\n",
    "# Step 3: 將 layer 目錄壓縮成 .zip 檔案\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk('./layer'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, arcname=os.path.relpath(file_path, './layer'))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{zip_filename} created successfully.\")\n",
    "\n",
    "\n",
    "# 上傳成 Lambda Layer\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "with open(zip_filename, 'rb') as f:\n",
    "    response = lambda_client.publish_layer_version(\n",
    "        LayerName=layer_name,\n",
    "        Description=layer_description,\n",
    "        Content={'ZipFile': f.read()},\n",
    "        CompatibleRuntimes=['python3.11'],  # 根據你的 Lambda 版本設定\n",
    "        LicenseInfo='MIT'  # 可選擇性設定 license\n",
    "    )\n",
    "\n",
    "fastapi_related_layer_arn = response[\"LayerVersionArn\"]\n",
    "\n",
    "# 顯示 Layer ARN\n",
    "print(\"Layer uploaded successfully:\")\n",
    "print(fastapi_related_layer_arn)\n",
    "\n",
    "# 清理 layer 資料夾\n",
    "shutil.rmtree('./layer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a35f98be-9d63-4a3a-94d4-db21f737737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "from sagemaker.workflow.functions import JsonGet # https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.functions.JsonGet\n",
    "\n",
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "streaming_response_function_name=\"lambda_streaming_response_function-\" + current_time\n",
    "\n",
    "lambda_web_adpter_layer_arn = f\"arn:aws:lambda:{sess.boto_region_name}:753240598075:layer:LambdaAdapterLayerX86:23\"\n",
    "pydantic_layer_arn = \"arn:aws:lambda:us-west-2:770693421928:layer:Klayers-p311-pydantic:10\"\n",
    "\n",
    "# 創建 Lambda 函數物件\n",
    "lambda_function = Lambda(\n",
    "    function_name=streaming_response_function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    zipped_code_dir=\"lambda_package.zip\",\n",
    "    handler=\"run.sh\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    "    runtime=\"python3.11\",\n",
    "    environment={\n",
    "        \"Variables\": {  \n",
    "            \"SAGEMAKER_ENDPOINT_NAME\": endpoint_name,\n",
    "            \"AWS_LAMBDA_EXEC_WRAPPER\": \"/opt/bootstrap\",\n",
    "            \"AWS_LWA_INVOKE_MODE\": \"response_stream\",\n",
    "            \"PORT\": \"8000\"\n",
    "        }\n",
    "    },\n",
    "    layers=[lambda_web_adpter_layer_arn, pydantic_layer_arn, fastapi_related_layer_arn]\n",
    ")\n",
    "\n",
    "# 創建 Lambda Step\n",
    "lambda_create_streaming_response_step = LambdaStep(\n",
    "    name=\"CreateStreamingResponseLambdaFunction\",\n",
    "    lambda_func=lambda_function,\n",
    "    depends_on=[lambda_deploy_step]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d730-af00-42d5-b74b-ec1a3c960625",
   "metadata": {},
   "source": [
    "## CreateLambdaFunctionURL Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09f9e4c6-e982-4635-a52d-f7d758b76344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_create_function_url.py\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    lambda_client = boto3.client('lambda')\n",
    "    function_name = event['function_name']\n",
    "    \n",
    "    # 創建 Lambda Function URL\n",
    "    response = lambda_client.create_function_url_config(\n",
    "        FunctionName=function_name,\n",
    "        AuthType='NONE',\n",
    "        InvokeMode='RESPONSE_STREAM'\n",
    "    )\n",
    "    \n",
    "    # 返回 Function URL\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': {\n",
    "            'function_url': response['FunctionUrl']\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a88cb7ba-a2d6-422c-aaf4-dc1bcd8ec3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "\n",
    "\n",
    "# 創建 Lambda 函數，用於創建 Function URL\n",
    "lambda_create_function_url = Lambda(\n",
    "    function_name=\"create_lambda_function_url\",\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"lambda_create_function_url.py\",\n",
    "    handler=\"lambda_create_function_url.lambda_handler\",\n",
    "    timeout=300,\n",
    "    memory_size=128\n",
    ")\n",
    "\n",
    "# 創建 LambdaStep\n",
    "create_lambda_function_url_step = LambdaStep(\n",
    "    name=\"CreateLambdaFunctionURL\",\n",
    "    lambda_func=lambda_create_function_url,\n",
    "    inputs={\n",
    "        \"function_name\": streaming_response_function_name\n",
    "    },\n",
    "    outputs=[\n",
    "        LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String),\n",
    "        LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "    ],\n",
    "    depends_on=[lambda_create_streaming_response_step]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e75618-de5a-4f05-9619-c48e06645c80",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98a0a9ec-6593-42d2-976d-bf2f4b9847dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"Training-Register-Deploy-ExposeEndpoint-Pipeline\",\n",
    "    parameters=[training_datasets_s3_uri, inference_instance_type],\n",
    "    steps=[train_step, register_step, lambda_deploy_step, lambda_create_streaming_response_step, create_lambda_function_url_step]\n",
    ")\n",
    "\n",
    "# 更新 SageMaker Pipeline (不存在則創建)\n",
    "pipeline.upsert(role_arn=role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3ecca-c6c7-4779-87e4-291a4f7c45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取得執行狀態\n",
    "# execution.describe()\n",
    "\n",
    "# # 等待 Pipeline 執行完成\n",
    "# execution.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb51bb6-5fcf-4192-af50-1f570151127b",
   "metadata": {},
   "source": [
    "## Manually Deploy a Model from the Registry\n",
    "Domumentation: https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-deploy.html#model-registry-deploy-smsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ae2c2-2530-4b7d-9240-8ac3181822e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.ModelPackage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_package_arn = \"arn:aws:sagemaker:us-west-2:097724924093:model-package/Demo-SageMaker-Pipeline-Group/3\"\n",
    "model = ModelPackage(role=role, # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.ModelPackage\n",
    "                     model_package_arn=model_package_arn,\n",
    "                     sagemaker_session=sess)\n",
    "model.deploy(initial_instance_count=1, instance_type='ml.g5.xlarge', container_startup_health_check_timeout=1000) # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21371a-5f11-4a79-9973-e1bcff7486f2",
   "metadata": {},
   "source": [
    "## Manually Deploy a Model from S3 (uncompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366d9f5-9b37-48c3-8766-887dde2806c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# Model s3 prefix(folder) uri  \n",
    "model_s3_prefix_uri = \"s3://sagemaker-us-west-2-097724924093/huggingface-qlora-microsoft-Phi-3-5-min-2024-09-11-04-03-17-319/output/model/\"\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "    'SM_NUM_GPUS': json.dumps(1), # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel( # https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n",
    "    model_data={'S3DataSource':{'S3Uri': model_s3_prefix_uri,'S3DataType': 'S3Prefix','CompressionType': 'None'}}, # We use a dict, for more details, please refer to these two documents: https://sagemaker.readthedocs.io/en/stable/api/inference/model.html, https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3ModelDataSource.html\n",
    "    role=role,\n",
    "    image_uri=llm_image,\n",
    "    sagemaker_session=sess,\n",
    "    env=config\n",
    ")\n",
    "\n",
    "\n",
    "model.deploy(initial_instance_count=1, instance_type='ml.g5.2xlarge', container_startup_health_check_timeout=1000) # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
