{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff841a2-306b-4a09-a248-d6dc01b94ec5",
   "metadata": {},
   "source": [
    "# Build a SageMaker Pipeline\n",
    "在這個部分，我們將一步步構建一個完整的 [SageMaker Pipeline](https://aws.amazon.com/sagemaker/pipelines/)，實現從模型訓練到部署的自動化流程。\n",
    "\n",
    "<img src=\"../imgs/pipeline.png\" alt=\"pipeline\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "Pipeline 將涵蓋從模型的訓練到部署的完整流程。具體來說，我們將進行以下操作：\n",
    "\n",
    "1. **TraingModel Step**: 使用 QLoRA fine-tune Phi-3.5-mini 模型，實現模型的定製化訓練。\n",
    "2. **RegisterModel Step**: 將訓練完成的模型註冊到 SageMaker 的 Model Registry，方便後續管理與版本控制。\n",
    "3. **LambdaDeployModel Step**: 使用 Lambda 來部署模型到 SageMaker Endpoint，讓模型能夠即時提供服務，並將 SageMaker Endpoint 與 API Gateway 整合，提供內部人員端點以取得模型推論結果。\n",
    "4. **CreateStreamingResponseLambdaFunction Step**: 創建一個 Lambda Function，通過 FastAPI 和 Lambda Web Adapter 實現流式回應，幫助處理來自 SageMaker Endpoint 的模型推論結果。\n",
    "5. **CreateLambdaFunctionURL Step**: 為流式回應的 Lambda Function 創建專屬的 Lambda Function URL，讓外部客戶端可以直接通過 URL 獲取流式數據。\n",
    "\n",
    "透過這個部分，你將實踐 MLOps 的任務，學習如何使用 SageMaker Pipeline 實現機器學習模型的訓練、自動化部署，以及如何將 Lambda 與 SageMaker 整合，提供流式回應 (Streaming Response) 給應用程式使用。\n",
    "\n",
    "\n",
    "<img src=\"../imgs/streaming-response.gif\" alt=\"pipeline\" width=\"400\"/>\n",
    "\n",
    "_API Reference: https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#steps_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bb6ca",
   "metadata": {},
   "source": [
    "## 配置環境\n",
    "\n",
    "在這個步驟，我們將安裝兩個重要的 Python 套件，這些是我們在整個 SageMaker Pipeline 中使用的核心工具：\n",
    "\n",
    "1. **sagemaker**: AWS 提供的官方 [SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/)，它能讓我們在 Jupyter Notebook 中直接與 SageMaker 互動，用來建立和管理訓練作業、模型部署等。\n",
    "2. **transformers**: 這是一個來自 [Hugging Face 的庫](https://huggingface.co/docs/transformers/index)，專門用於自然語言處理（NLP）模型的訓練與使用。在這個工作坊中，我們會使用它來 fine-tune 預訓練模型。\n",
    "\n",
    "> 我們將指定安裝 `transformers==4.44.2` 版本來確保與範例代碼的相容性。\n",
    "\n",
    "執行這段程式碼後，你的環境會安裝這些必要的套件，並準備好進行接下來的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4581744-4b63-4dc9-b506-4c6ca9e6c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker transformers==4.44.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10393a74",
   "metadata": {},
   "source": [
    "在這個步驟中，我們將進行 SageMaker 環境的初始化工作。這些操作對於接下來的 SageMaker Pipeline 非常重要，因為它們確保我們有正確的權限與資源來運行所有的步驟。\n",
    "\n",
    "以下是這段程式碼的核心功能：\n",
    "1. **引入所需的模組**: 我們導入了用於構建 SageMaker Pipeline 的一系列模組，包括 `TrainingStep`、`ProcessingStep`、`CreateModelStep` 和 `ParameterString` 等，這些將幫助我們定義和管理 Pipeline 中的各個步驟。\n",
    "2. **設定 SageMaker Session**: 使用 `sagemaker.Session()` 來啟動一個 SageMaker Session，這個 session 是我們與 SageMaker 互動的橋樑。我們還設置了 S3 bucket 作為存放模型和資料的默認位置。如果沒有指定 bucket，則使用 SageMaker 的默認 bucket。\n",
    "3. **獲取執行角色 (Execution Role)**: 我們使用 `sagemaker.get_execution_role()` 來獲取執行 SageMaker 操作所需的 IAM 角色，這個角色賦予必要的權限來執行訓練和部署任務。如果自動獲取角色失敗，會手動指定一個預設的 SageMaker 執行角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4382b5-28fb-4434-a3c1-88bd6ad252b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import TrainingStep, ProcessingStep, CreateModelStep, CacheConfig\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828512d-0375-4104-b71c-9769f7f7b0a8",
   "metadata": {},
   "source": [
    "## Pipeline Parameters\n",
    "\n",
    "在建立 Pipeline 時，我們可以定義一些參數，以便每次執行時，可以動態地傳入一些配置以便根據不同的需求進行設定與調整，這些參數要在建立 Pipeline 時定義。\n",
    "\n",
    "<img src=\"../imgs/pipeline-parameters.png\" alt=\"pipeline-parameters\" width=\"400\"/>\n",
    "\n",
    "1. **定義 Pipeline 參數**:\n",
    "   - `inference_instance_type`: 這個參數定義了推理所需的實例類型，默認值為 `\"ml.g5.xlarge\"`，這是一個高性能的 GPU 實例，適合處理大型模型的推論工作。\n",
    "   - `model_artifact_s3_uri`: 這個參數定義了模型檔案的 S3 路徑位置。\n",
    "\n",
    "2. **配置其他 Pipeline 設定**:\n",
    "   - `model_package_group_name`: 這裡定義了模型註冊所屬的群組名稱，我們將其設為 `\"Demo-SageMaker-Pipeline-Group\"`，便於後續在 Model Registry 中進行模型版本管理。\n",
    "   - `cache_config`: 這是 SageMaker Pipeline 中的快取配置。通過啟用快取，我們可以在相同的步驟沒有變化時跳過重複運行，從而加快 pipeline 的執行速度。在此例中，我們設置快取的有效期限為 30 天（`\"30d\"`），也就是在這段時間內，如果該步驟的輸入沒有變化，將不會重新運行。\n",
    "\n",
    "這些參數與配置為我們的 pipeline 提供了靈活性和效率，讓模型訓練與推理可以根據需求動態調整，同時提升了 pipeline 的運行效能。\n",
    "\n",
    "_API Referecne: https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fe262-89b9-4c0e-90a8-596cd554eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義參數\n",
    "inference_instance_type = ParameterString(name=\"InferenceInstanceType\", default_value=\"ml.g5.xlarge\")\n",
    "model_artifact_s3_uri = ParameterString(name=\"ModelArtifactS3Uri\")\n",
    "model_name = ParameterString(name=\"ModelName\", default_value=\"psy-1-model-created-at-2024-\")\n",
    "random_string = ParameterString(name=\"RandomString\")\n",
    "\n",
    "# 其他配置\n",
    "model_package_group_name = \"Demo-SageMaker-Pipeline-Group\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b536da2-8cfe-4f15-a132-e71fcba80a04",
   "metadata": {},
   "source": [
    "## 定義 Register Model Step\n",
    "\n",
    "### 什麼是 Model Registry？\n",
    "\n",
    "在定義前，我們先來了解一下 [Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html) 是什麼。\n",
    "\n",
    "Model Registry 是 SageMaker 提供的一個功能，幫助我們管理和追蹤機器學習模型的不同版本。當我們在模型訓練完成後，經常會需要對模型進行版本控制、審核與部署，Model Registry 正是為這些需求設計的。透過 Model Registry，我們可以：\n",
    "- **模型版本控制**：每次訓練完成後，可以將模型註冊到 Model Registry 中，這樣我們可以方便地管理多個版本的模型。\n",
    "- **審核與批准流程**：模型可以設置不同的狀態，例如 Pending、Approved 或 Rejected，幫助我們管理模型的生命週期。\n",
    "- **便於部署**：一旦模型被註冊，我們可以快速將它部署到 SageMaker Endpoint，並且利用不同實例來處理推理請求。\n",
    "\n",
    "### 使用 RegisterModel Step\n",
    "\n",
    "Document: https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-register-model\n",
    "\n",
    "這段程式碼展示了如何在 SageMaker Pipeline 中使用 `RegisterModel` 步驟，將訓練完成的模型註冊到 Model Registry 中。\n",
    "\n",
    "1. **取得 LLM 映像 URI**：\n",
    "   - 我們使用 `get_huggingface_llm_image_uri()` 來獲取 Hugging Face 大型語言模型 (LLM) 的 Image URI，這是用來推理時的 Docker Image，確保模型可以在 SageMaker Endpoint 上正確運行。\n",
    "\n",
    "2. **定義模型配置**：\n",
    "   - `config` 變數定義了推理過程中的一些環境配置，比如最大輸入長度（`MAX_INPUT_LENGTH`）和使用的 GPU 數量（`SM_NUM_GPUS`）。這些配置將在模型註冊時被應用，確保推理環境和訓練時的條件一致。\n",
    "\n",
    "3. **創建 Hugging Face 模型**：\n",
    "   - 使用 `HuggingFaceModel` 來將訓練好的模型包裝成一個 SageMaker 模型。這裡，我們通過 `train_step.properties.ModelArtifacts.S3ModelArtifacts` 來指定模型的 S3 路徑，這個路徑是訓練步驟中產生的模型檔案存放位置。\n",
    "   - 我們還指定了推理時需要的 Docker 映像 URI、執行角色、SageMaker Session 和環境變數。\n",
    "\n",
    "4. **註冊模型**：\n",
    "   - 使用 `RegisterModel` 步驟將模型註冊到 Model Registry 中。這裡，我們指定了模型的 `content_types` 和 `response_types` 來定義模型可以處理的輸入與輸出格式（如 `application/json`），並且定義了推理實例類型（`inference_instance_type`）。\n",
    "   - 我們還將模型註冊到一個名為 `\"Demo-SageMaker-Pipeline-Group\"` 的模型包群組中，這有助於進行版本管理。最後，將 `approval_status` 設為 `\"Approved\"`，表示這個模型是已審核通過的，可以直接部署。\n",
    "\n",
    "透過這個步驟，我們可以將訓練完成的模型保存到 SageMaker 的 Model Registry 中，方便進行版本控制與部署。這是 MLOps 的一個重要部分，能確保我們的模型管理更加有條理且可追蹤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd3e52-9dac-4f51-885a-bf8dab1bd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "    'SM_NUM_GPUS': json.dumps(1), # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel( # https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model\n",
    "    model_data={'S3DataSource':{'S3Uri': model_artifact_s3_uri,'S3DataType': 'S3Prefix','CompressionType': 'None'}}, # https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html\n",
    "    role=role,\n",
    "    image_uri=llm_image,\n",
    "    sagemaker_session=sess,\n",
    "    env=config\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[inference_instance_type],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829532f-15ed-42dc-853c-e4e7c9f4d2ac",
   "metadata": {},
   "source": [
    "## 定義 Lambda Step for Deploying model endpoint\n",
    "\n",
    "接下來我們要來定義 Lambda Step，這個步驟將會使用 Lambda Function 來部署模型到 SageMaker Endpoint，讓模型能夠即時提供服務。\n",
    "我們會先撰寫一個 `lambda_deployer.py` 檔案，這個檔案將會被 Lambda Function 使用\n",
    "\n",
    "documents: \n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-lambda\n",
    "- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed048d2",
   "metadata": {},
   "source": [
    "### Lambda 部署模型與 API Gateway 創建\n",
    "\n",
    "在這段程式碼中，我們定義了一個 Lambda function，目的是將 SageMaker 模型部署到 SageMaker Endpoint 並自動創建一個 API Gateway，讓外部用戶端可以通過 API 調用這個端點。\n",
    "\n",
    "#### 主要步驟如下：\n",
    "\n",
    "1. **Lambda Function 輸入參數**：\n",
    "   - Lambda 使用 `event` 中傳入的參數，包括模型的 ARN、端點名稱、角色 ARN 等，以動態設定模型部署的細節。\n",
    "\n",
    "2. **創建 SageMaker 模型**：\n",
    "   - 使用 `boto3` 的 `sagemaker` 客戶端，調用 `create_model()` 函數來創建一個新的 SageMaker 模型。這個模型會根據提供的 `model_package_arn` 和 IAM 執行角色進行配置。\n",
    "\n",
    "3. **創建 Endpoint 配置與 Endpoint**：\n",
    "   - `create_endpoint_config()` 創建端點的配置，定義了 SageMaker 如何處理流量的實例類型與數量。\n",
    "   - 然後，透過 `create_endpoint()` 創建實際的 SageMaker Endpoint，該端點將用來進行模型推理。\n",
    "\n",
    "4. **創建 API Gateway**：\n",
    "   - 使用 `apigateway_client` 創建一個新的 API Gateway，並定義它將會處理的 HTTP 請求方法（POST）。API Gateway 將通過 SageMaker Runtime API 來調用 SageMaker 端點，實現與模型的整合。\n",
    "   - 通過 `put_integration()` 函數，我們將 API Gateway 與 SageMaker Runtime 進行整合，這使得 API Gateway 能夠直接向 SageMaker 發送推理請求。\n",
    "\n",
    "5. **部署 API Gateway**：\n",
    "   - 創建 API Gateway 的資源、方法和整合配置後，透過 `create_deployment()` 將 API 部署到 `\"dev\"` 階段，並生成 API 的可調用 URL。\n",
    "\n",
    "6. **返回結果**：\n",
    "   - Lambda 最後返回 API Gateway 的 URL 及端點名稱，表示模型部署和 API Gateway 創建成功。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65fb8e-a048-4019-8e30-e0475271ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_deployer.py\n",
    "\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "\n",
    "import time\n",
    "\n",
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%H-%M\", time.localtime())\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Lambda function to deploy a model to an Endpoint using boto3 and create an API Gateway for SageMaker\"\"\"\n",
    "\n",
    "    # 使用 event 中的參數\n",
    "    random_string = event[\"random_string\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    model_name = event[\"model_name\"] + \"-run-at-\" + current_time + \"-\" + random_string\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    endpoint_config_name = endpoint_name + \"-\" + current_time + \"-\" + random_string\n",
    "    role = event[\"role\"]\n",
    "    apigateway_role = event[\"apigateway_role\"]\n",
    "    inference_instance_type = event[\"inference_instance_type\"]\n",
    "\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "    apigateway_client = boto3.client(\"apigateway\")\n",
    "\n",
    "    # 創建 SageMaker 模型\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\"ModelPackageName\": model_package_arn},\n",
    "        ExecutionRoleArn=role,\n",
    "    )\n",
    "\n",
    "    # 創建端點配置\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": inference_instance_type,\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 創建端點\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "    # 創建 API Gateway\n",
    "    api_name = f\"{model_name}-api\"\n",
    "    create_api_response = apigateway_client.create_rest_api(\n",
    "        name=api_name,\n",
    "        description=f\"API Gateway for SageMaker endpoint {endpoint_name}\",\n",
    "        endpointConfiguration={\"types\": [\"REGIONAL\"]},\n",
    "    )\n",
    "\n",
    "    # 取得 API Gateway 的根資源 ID\n",
    "    api_id = create_api_response[\"id\"]\n",
    "    root_id = apigateway_client.get_resources(restApiId=api_id)[\"items\"][0][\"id\"]\n",
    "\n",
    "    # 創建 POST 方法並設定\n",
    "    apigateway_client.put_method(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        authorizationType=\"NONE\",\n",
    "    )\n",
    "\n",
    "    # 設置 SageMaker Runtime 與 API Gateway 的整合\n",
    "    apigateway_client.put_integration(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        type=\"AWS\",  # 使用 AWS Service Integration\n",
    "        integrationHttpMethod=\"POST\",\n",
    "        uri=f\"arn:aws:apigateway:{boto3.Session().region_name}:runtime.sagemaker:path/endpoints/{endpoint_name}/invocations\",\n",
    "        credentials=apigateway_role,  # 指定具有 SageMaker InvokeEndpoint 權限的角色\n",
    "    )\n",
    "\n",
    "    # 設置 Integration Response，以便 API Gateway 正確處理 SageMaker 回應\n",
    "    apigateway_client.put_integration_response(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        statusCode=\"200\",\n",
    "        responseTemplates={\"application/json\": \"$input.body\"},\n",
    "    )\n",
    "\n",
    "    # 創建方法回應，確保有適當的狀態碼\n",
    "    apigateway_client.put_method_response(\n",
    "        restApiId=api_id,\n",
    "        resourceId=root_id,\n",
    "        httpMethod=\"POST\",\n",
    "        statusCode=\"200\",\n",
    "        responseModels={\"application/json\": \"Empty\"},\n",
    "    )\n",
    "\n",
    "    # 部署 API Gateway\n",
    "    apigateway_client.create_deployment(\n",
    "        restApiId=api_id,\n",
    "        stageName=\"dev\",\n",
    "    )\n",
    "\n",
    "    # 返回 API Gateway 的 URL\n",
    "    api_url = (\n",
    "        f\"https://{api_id}.execute-api.{boto3.Session().region_name}.amazonaws.com/dev\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\n",
    "            {\n",
    "                \"message\": f\"Created Endpoint {endpoint_name} and API Gateway {api_name}!\",\n",
    "                \"endpoint_name\": endpoint_name,\n",
    "                \"api_url\": api_url,\n",
    "            }\n",
    "        ),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f67bbd",
   "metadata": {},
   "source": [
    "### 建立 Lambda 和 API Gateway 所需的 IAM Role\n",
    "\n",
    "在這段程式碼中，我們定義了一些用來建立和配置 IAM 角色 (Role) 的函數，這些角色將會賦予 Lambda 和 API Gateway 所需的權限，讓它們能夠調用 SageMaker 端點並執行必要的操作。\n",
    "\n",
    "#### 這段程式碼的目的是：\n",
    "- 自動化 IAM 角色的創建與配置，確保 Lambda 和 API Gateway 擁有正確的權限來進行 SageMaker 端點的部署和調用。\n",
    "- 避免重複附加策略，確保 IAM 角色的權限管理保持簡潔而不冗餘。\n",
    "\n",
    "透過這些 IAM 角色的設定，我們能夠確保模型的部署與推理請求能順利執行，並且滿足各個服務的權限要求。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe4780-fe3e-4279-9d10-a64cd1d56756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "def attach_policy_if_missing(role_name, policy_arn):\n",
    "    \"\"\"檢查並附加策略到角色，若策略已存在則跳過\"\"\"\n",
    "    try:\n",
    "        # 獲取附加到角色的現有策略\n",
    "        attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        attached_policy_arns = [policy['PolicyArn'] for policy in attached_policies]\n",
    "\n",
    "        # 如果策略不在已附加列表中，則附加\n",
    "        if policy_arn not in attached_policy_arns:\n",
    "            iam.attach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "            print(f\"Attached policy {policy_arn} to role {role_name}\")\n",
    "        else:\n",
    "            print(f\"Policy {policy_arn} already attached to role {role_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy {policy_arn} to role {role_name}: {e}\")\n",
    "\n",
    "def create_lambda_role(role_name, apigateway_role_arn):\n",
    "    try:\n",
    "        # 創建 IAM 角色\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to interact with SageMaker, API Gateway, and Bedrock'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    # 無論角色是否已存在，都會檢查並附加所需的策略\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole')\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess')\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/AmazonAPIGatewayAdministrator')\n",
    "\n",
    "    # 附加 Lambda Function URL 創建的策略\n",
    "    policy_document_url_config = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \"lambda:CreateFunctionUrlConfig\",\n",
    "                \"Resource\": f\"arn:aws:lambda:*:{role_arn.split(':')[4]}:function:*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=\"LambdaURLConfigPolicy\",\n",
    "        PolicyDocument=json.dumps(policy_document_url_config)\n",
    "    )\n",
    "\n",
    "    # 在 Lambda 角色中附加 iam:PassRole 權限，允許傳遞 API Gateway 的角色\n",
    "    policy_document_pass_role = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \"iam:PassRole\",\n",
    "                \"Resource\": apigateway_role_arn\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=\"PassRolePolicy\",\n",
    "        PolicyDocument=json.dumps(policy_document_pass_role)\n",
    "    )\n",
    "\n",
    "    # 附加 Bedrock 權限\n",
    "    bedrock_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                    \"bedrock:ListModels\",\n",
    "                    \"bedrock:InvokeModelWithResponseStream\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=\"BedrockInvokePolicy\",\n",
    "        PolicyDocument=json.dumps(bedrock_policy_document)\n",
    "    )\n",
    "\n",
    "    print(f\"Attached iam:PassRole, lambda:CreateFunctionUrlConfig, and Bedrock policies to {role_name}\")\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "\n",
    "def create_apigateway_role(role_name):\n",
    "    try:\n",
    "        # 創建 API Gateway 用的 IAM 角色\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"apigateway.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for API Gateway to invoke SageMaker Endpoints'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "    # 附加允許 API Gateway 調用 SageMaker 的策略\n",
    "    attach_policy_if_missing(role_name, 'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess')\n",
    "\n",
    "    return role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ecd57",
   "metadata": {},
   "source": [
    "### 定義 LambdaDeployModelEndpoint Step\n",
    "\n",
    "這段程式碼展示了如何將模型的部署過程納入 SageMaker Pipeline，並且使用 Lambda 來自動化模型的部署、端點的創建以及 API Gateway 的設定。Lambda Step 是 SageMaker Pipeline 中一個靈活的步驟，它允許我們在 Pipeline 中插入自定義的 Lambda function，來完成一些 SageMaker 本身無法直接處理的操作。\n",
    "\n",
    "![apigw-sagemaker-endpoint](../imgs/apigw-sagemaker-endpoint.png)\n",
    "\n",
    "#### 主要功能和步驟如下：\n",
    "\n",
    "1. **建立所需的 IAM 角色**：\n",
    "   - 首先，我們使用 `create_apigateway_role()` 和 `create_lambda_role()` 來創建 API Gateway 和 Lambda 所需的 IAM 角色，這些角色賦予它們調用 SageMaker 端點的權限。\n",
    "\n",
    "2. **定義 Lambda 相關資源名稱**：\n",
    "   - 透過 `time.strftime()` 獲取當前時間，並將其添加到資源名稱中，確保每次創建的模型、端點、配置和 Lambda 函數都有獨特的名稱，避免命名衝突。\n",
    "\n",
    "3. **Lambda 建立與配置**：\n",
    "   - 使用 `sagemaker.lambda_helper.Lambda` 來定義 Lambda function，這個函數將運行 `lambda_deployer.py` 腳本中的邏輯，負責創建 SageMaker 模型、端點、以及 API Gateway。\n",
    "   - 我們設置了 Lambda 的基本配置，如執行角色 (`execution_role_arn`)、處理器 (`handler`)、腳本 (`script`)、記憶體大小 (`memory_size`) 和超時時間 (`timeout`)，確保它能夠順利運行所有需要的步驟。\n",
    "\n",
    "4. **捕捉 Lambda 的輸出**：\n",
    "   - 使用 `LambdaOutput` 來捕捉 Lambda function 的輸出，這些輸出會返回給 Pipeline。每個 `LambdaOutput` 對應 Lambda 返回的字典中的一個鍵值，這裡我們捕捉了 `statusCode` 和 `body` 這兩個參數，來確認 Lambda function 的執行狀況。\n",
    "\n",
    "5. **定義 LambdaStep 部署模型端點**：\n",
    "   - `LambdaStep` 是 SageMaker Pipeline 中用來調用 Lambda function 的核心部分。在這裡，我們將 LambdaStep 設置為名為 `\"LambdaStepDeployModelEndpoint\"` 的步驟，並傳入所需的 `inputs`，例如模型的 ARN、模型名稱、端點配置名稱、端點名稱以及 Lambda 和 API Gateway 的角色。\n",
    "   - `inputs` 是通過 `event` 對象傳入到 Lambda function 中，這樣 Lambda 就可以根據這些輸入動態創建 SageMaker 模型和端點，並配置 API Gateway。\n",
    "\n",
    "### 這段程式碼的目的：\n",
    "- 使用 Lambda function 自動化模型的部署過程，這包括創建模型、配置端點、並生成 API Gateway 來讓外部系統調用 SageMaker 模型進行推理。\n",
    "- 利用 `LambdaStep` 將 Lambda function 納入 SageMaker Pipeline 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b202ea-386c-42c4-b9fd-cb3763b36ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.workflow.functions import JsonGet, Join\n",
    "\n",
    "\n",
    "apigateway_role = create_apigateway_role(\"apigateway-role\")\n",
    "lambda_role = create_lambda_role(\"lambda-deployment-role\", apigateway_role_arn=apigateway_role)\n",
    "\n",
    "\n",
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "# YOU CAN NOT concatenate the pipeline variables using Python primitives \n",
    "# https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html#not-all-built-in-python-operations-can-be-applied-to-parameters\n",
    "# Instead, please use sagemaker.workflow.functions.Join(), for more details, visit: https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.functions.Join\n",
    "endpoint_name = \"pipeline-endpoint\"\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=\"endpoint-deployer\",\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function\n",
    "# in the Lambda\n",
    "lambda_deploy_step = LambdaStep(\n",
    "    name=\"LambdaStepDeployModelEndpoint\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_package_arn\": register_step.properties.ModelPackageArn,\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"random_string\": random_string,\n",
    "        \"role\": role,\n",
    "        \"apigateway_role\": apigateway_role,\n",
    "        \"inference_instance_type\": inference_instance_type\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0ac9e-9476-48db-ba5c-53a0cf6279d9",
   "metadata": {},
   "source": [
    "## 定義 CreateStreamingResponseLambdaFunction Step\n",
    "\n",
    "我們將創建一個 Lambda Function，這個 Lambda 使用 [Lambda Web Adapter](https://github.com/awslabs/aws-lambda-web-adapter) 結合 FastAPI 作為框架。其核心業務邏輯是利用 `boto3` 調用 `sagemaker_runtime.invoke_endpoint_with_response_stream`，從 SageMaker 端點獲取推理結果，並通過流式回應 (Streaming Response) 的方式將結果回傳給調用 Lambda Function 的用戶端。\n",
    "\n",
    "![lambda-web-adatper](../imgs/lambda-web-adapter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960367d9",
   "metadata": {},
   "source": [
    "### Streaming Response Lambda Function 的初始化\n",
    "\n",
    "在這段程式碼中，我們使用 FastAPI 來構建一個 Lambda Function，它能夠處理用戶端發送的推理請求，並使用流式回應（Streaming Response）的方式將結果返回。這個 Lambda Function 可以與 SageMaker 端點和 Bedrock 服務進行整合，實現流式處理大規模模型的回應。\n",
    "\n",
    "#### 主要功能包括：\n",
    "1. **FastAPI 框架**: 我們使用 FastAPI 來設置 HTTP 路由和處理推理請求。這是一個輕量且高效的 Python Web 框架，特別適合在 Lambda 上運行。\n",
    "2. **CORS 設定**: 由於會使用到瀏覽器來跨域請求，這裡配置了 CORS (Cross-Origin Resource Sharing) 允許來自不同網域的請求。\n",
    "3. **前端 Demo 頁面配置**: 我們將前端 Demo 頁面掛載到 `/demo` 路徑，並設置一個根路由來將請求重定向到這個頁面。\n",
    "4. **Bedrock 與 SageMaker 整合**: 這段程式碼定義了如何將用戶端的請求轉換為 Bedrock 和 SageMaker 端點的推理請求，並使用流式回應的方式將模型生成的結果回傳給用戶端。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914fe446-a8c4-43d1-89e0-0a119fd83e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import boto3\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import RedirectResponse, StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-runtime\")\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "SAGEMAKER_ENDPOINT_NAME = os.getenv(\"SAGEMAKER_ENDPOINT_NAME\")\n",
    "\n",
    "# CORS 設定\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "app.mount(\"/demo\", StaticFiles(directory=\"static\", html=True))\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return RedirectResponse(url=\"/demo/\")\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str  # Role can be 'user' or 'assistant'\n",
    "    content: str  # The content of the message\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    model: str  # Model name provided by the client\n",
    "    system: Optional[str] = None  # Optional system prompt\n",
    "    messages: List[Message]  # List of messages with roles and content\n",
    "    temperature: Optional[float] = 0.5  # Optional, default temperature is 0.5\n",
    "    max_tokens: Optional[int] = 1024  # Optional, default max_tokens is 1024\n",
    "    stream: Optional[bool] = True  # Enable streaming by default\n",
    "\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "def api_chat_completion(chat_request: ChatRequest):\n",
    "    if not chat_request.messages:\n",
    "        return {\"error\": \"Messages are required\"}\n",
    "\n",
    "    # 如果 model 是 'psy-1'，調用 SageMaker 端點\n",
    "    if chat_request.model == \"psy-1\":\n",
    "        body = {\n",
    "            \"inputs\": f\"<|system|>{chat_request.system or ''}<|end|><|user|>{chat_request.messages[-1].content}<|end|><|assistant|>\",\n",
    "            \"parameters\": {\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": chat_request.temperature,\n",
    "                \"max_new_tokens\": chat_request.max_tokens,\n",
    "                \"repetition_penalty\": 1.03,\n",
    "                \"stop\": [\"\\nUser:\", \"<|endoftext|>\", \"###\"],\n",
    "            },\n",
    "            \"stream\": chat_request.stream,\n",
    "        }\n",
    "        return StreamingResponse(\n",
    "            sagemaker_stream(\n",
    "                SAGEMAKER_ENDPOINT_NAME, body\n",
    "            ),\n",
    "            media_type=\"text/html\",\n",
    "        )\n",
    "\n",
    "    # Default to Bedrock model\n",
    "    body = {\n",
    "        \"max_tokens\": chat_request.max_tokens,  # Accept max_tokens from the front-end\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",  # Required by Bedrock API\n",
    "        \"messages\": [\n",
    "            {\"role\": msg.role, \"content\": msg.content} for msg in chat_request.messages\n",
    "        ],\n",
    "        \"temperature\": chat_request.temperature,  # Accept temperature from the front-end\n",
    "    }\n",
    "\n",
    "    # Include the system prompt if provided\n",
    "    if chat_request.system:\n",
    "        body[\"system\"] = chat_request.system\n",
    "\n",
    "    return StreamingResponse(\n",
    "        bedrock_stream(chat_request.model, body), media_type=\"text/html\"\n",
    "    )\n",
    "\n",
    "async def bedrock_stream(model_id: str, body: dict):\n",
    "    # Convert the dictionary into a JSON string\n",
    "    body_str = json.dumps(body)\n",
    "\n",
    "    # Send the model ID from the request and the body to Bedrock\n",
    "    response = bedrock.invoke_model_with_response_stream(\n",
    "        modelId=model_id,  # Model name provided in the request body\n",
    "        body=body_str,\n",
    "    )\n",
    "\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                message = json.loads(chunk.get(\"bytes\").decode())\n",
    "                if message[\"type\"] == \"content_block_delta\":\n",
    "                    # Stream the content back to the client\n",
    "                    yield message[\"delta\"][\"text\"] or \"\"\n",
    "                elif message[\"type\"] == \"message_stop\":\n",
    "                    # Indicate the end of the message\n",
    "                    yield \"\\n\"\n",
    "\n",
    "async def sagemaker_stream(endpoint_name: str, body: dict):\n",
    "    body_str = json.dumps(body)\n",
    "\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=body_str,\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "    stream = response['Body']\n",
    "    complete_text = \"\"  # 用來存儲最終的合併文本\n",
    "    incomplete_message = \"\"  # 用來存儲不完整的消息\n",
    "\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'PayloadPart' in event:\n",
    "                try:\n",
    "                    raw_message = event['PayloadPart']['Bytes']\n",
    "                    # 設定解碼錯誤策略，允許跳過無效字元\n",
    "                    decoded_message = raw_message.decode('utf-8', errors='ignore')\n",
    "                    print(f\"Raw Message after stripping 'data:': {decoded_message}\")\n",
    "\n",
    "                    if decoded_message.startswith(\"data:\"):\n",
    "                        decoded_message = decoded_message[5:].strip()\n",
    "\n",
    "                    # 將不完整的訊息拼接起來\n",
    "                    if incomplete_message:\n",
    "                        decoded_message = incomplete_message + decoded_message\n",
    "                        incomplete_message = \"\"\n",
    "\n",
    "                    # 嘗試解析為 JSON\n",
    "                    try:\n",
    "                        json_message = json.loads(decoded_message)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # 如果 JSON 解析失敗，將訊息暫存並等待下一個 event\n",
    "                        incomplete_message = decoded_message\n",
    "                        continue\n",
    "\n",
    "                    # 確認 token 存在且非特殊字符\n",
    "                    token = json_message.get('token', {}).get('text')\n",
    "                    special = json_message.get('token', {}).get('special', False)\n",
    "\n",
    "                    # 檢查 token 是否是 \"<|end|>\" 或 special 為 true，直接終止流\n",
    "                    if token == \"<|end|>\" or special:\n",
    "                        print(\"End token detected, stopping stream.\")\n",
    "                        break  # 結束迴圈並停止拼接\n",
    "\n",
    "                    if token and token.strip():\n",
    "                        complete_text += token  # 拼接完整的 text\n",
    "                        yield token  # 實時回傳 token\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"JSON Decode Error: Skipping invalid JSON data.\")\n",
    "                    continue\n",
    "\n",
    "            elif 'ModelStreamError' in event:\n",
    "                error_message = event['ModelStreamError']['Message']\n",
    "                print(f\"Model stream error: {error_message}\")\n",
    "                # 過濾錯誤訊息，不傳回給前端\n",
    "                continue\n",
    "\n",
    "            elif 'InternalStreamFailure' in event:\n",
    "                print(f\"Internal stream failure: {event['InternalStreamFailure']['Message']}\")\n",
    "                # 過濾錯誤訊息，不傳回給前端\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", \"8080\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0efa14",
   "metadata": {},
   "source": [
    "`run.sh` 用於啟動 Lambda 中的 FastAPI 應用，同時也將會被指定為 Lambda Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cf242-16b4-4842-9257-45603a01fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "PATH=$PATH:$LAMBDA_TASK_ROOT/bin \\\n",
    "    PYTHONPATH=$PYTHONPATH:/opt/python:$LAMBDA_RUNTIME_DIR \\\n",
    "    exec python -m uvicorn --port=$PORT main:app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517cd46-7e37-468c-8307-207578d7c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593f9e8-a562-44ee-a9a5-822bd29474b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile static/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Psy test demo</title>\n",
    "    <link rel=\"stylesheet\" href=\"style.css\" />\n",
    "    <link\n",
    "      rel=\"stylesheet\"\n",
    "      href=\"https://fonts.googleapis.com/css?family=Roboto:300,300italic,700,700italic\"\n",
    "    />\n",
    "    <link\n",
    "      rel=\"stylesheet\"\n",
    "      href=\"https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.css\"\n",
    "    />\n",
    "    <link\n",
    "      rel=\"stylesheet\"\n",
    "      href=\"https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.1/milligram.css\"\n",
    "    />\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <div id=\"container\" class=\"row\">\n",
    "      <div class=\"column column-67\">\n",
    "        <h1>Psy test demo</h1>\n",
    "        <h4>\n",
    "          Enter your request details below and let the AI generate a response.\n",
    "        </h4>\n",
    "\n",
    "        <!-- Input fields for model, system prompt, and user message -->\n",
    "        <label for=\"model\">Model:</label>\n",
    "        <small\n",
    "          ><a\n",
    "            href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html\"\n",
    "            target=\"_blank\"\n",
    "            >Refer to the AWS Bedrock model IDs documentation</a\n",
    "          ></small\n",
    "        >\n",
    "        <input\n",
    "          type=\"text\"\n",
    "          id=\"model\"\n",
    "          placeholder=\"Enter model ID (e.g., 'anthropic.claude-3-sonnet-20240229-v1:0')\"\n",
    "          value=\"psy-1\"\n",
    "        />\n",
    "\n",
    "        <label for=\"system\">System Prompt:</label>\n",
    "        <textarea id=\"system\" placeholder=\"Enter system prompt\"></textarea>\n",
    "\n",
    "        <label for=\"user-message\">User Message:</label>\n",
    "        <textarea id=\"user-message\" placeholder=\"Enter user message\"></textarea>\n",
    "\n",
    "        <!-- Input for max_tokens and temperature -->\n",
    "        <label for=\"max-tokens\">Max Tokens:</label>\n",
    "        <input\n",
    "          type=\"number\"\n",
    "          id=\"max-tokens\"\n",
    "          value=\"1024\"\n",
    "          min=\"1\"\n",
    "          placeholder=\"Enter max tokens\"\n",
    "        />\n",
    "\n",
    "        <label for=\"temperature\">Temperature:</label>\n",
    "        <input\n",
    "          type=\"number\"\n",
    "          step=\"0.1\"\n",
    "          id=\"temperature\"\n",
    "          value=\"0.5\"\n",
    "          min=\"0\"\n",
    "          max=\"1\"\n",
    "          placeholder=\"Enter temperature\"\n",
    "        />\n",
    "\n",
    "        <!-- Button to trigger the API call -->\n",
    "        <button id=\"generate-response\">Generate</button>\n",
    "\n",
    "        <!-- Output area for the story -->\n",
    "        <div id=\"story-output\"></div>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "    <script src=\"script.js\"></script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2133cb4-ad2f-4406-ab90-933a766ee9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile static/script.js\n",
    "\n",
    "async function generateAIResponse() {\n",
    "  // Get input values from the form\n",
    "  const model = document.getElementById(\"model\").value;\n",
    "  const system = document.getElementById(\"system\").value;\n",
    "  const userMessage = document.getElementById(\"user-message\").value;\n",
    "  const maxTokens = document.getElementById(\"max-tokens\").value;\n",
    "  const temperature = document.getElementById(\"temperature\").value;\n",
    "\n",
    "  if (userMessage.trim().length === 0) {\n",
    "    return;\n",
    "  }\n",
    "\n",
    "  const storyOutput = document.getElementById(\"story-output\");\n",
    "  storyOutput.innerText = \"Thinking...\";\n",
    "\n",
    "  try {\n",
    "    // Create request payload\n",
    "    const requestBody = {\n",
    "      model: model,\n",
    "      system: system,\n",
    "      messages: [{\n",
    "        role: 'user',\n",
    "        content: userMessage\n",
    "      }],\n",
    "      max_tokens: parseInt(maxTokens),\n",
    "      temperature: parseFloat(temperature),\n",
    "      stream: true\n",
    "    };\n",
    "\n",
    "    // Use Fetch API to send a POST request for response streaming\n",
    "    const response = await fetch(\"/v1/chat/completions\", {\n",
    "      method: \"POST\",\n",
    "      headers: {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "      },\n",
    "      body: JSON.stringify(requestBody)\n",
    "    });\n",
    "\n",
    "    storyOutput.innerText = \"\";\n",
    "\n",
    "    // Response Body is a ReadableStream\n",
    "    const reader = response.body.getReader();\n",
    "    const decoder = new TextDecoder();\n",
    "\n",
    "    // Process the chunks from the stream\n",
    "    while (true) {\n",
    "      const {\n",
    "        done,\n",
    "        value\n",
    "      } = await reader.read();\n",
    "      if (done) {\n",
    "        break;\n",
    "      }\n",
    "      const text = decoder.decode(value);\n",
    "      storyOutput.innerText += text;\n",
    "    }\n",
    "\n",
    "  } catch (error) {\n",
    "    storyOutput.innerText = `Sorry, an error happened. Please try again later. \\n\\n ${error}`;\n",
    "  }\n",
    "}\n",
    "\n",
    "document.getElementById(\"generate-response\").addEventListener(\"click\", generateAIResponse);\n",
    "document.getElementById('user-message').addEventListener('keydown', function (e) {\n",
    "  if (e.code === 'Enter') {\n",
    "    generateAIResponse();\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2ff63-d641-4912-91fb-d7173d0c934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile static/style.css\n",
    "\n",
    "body {\n",
    "    font-family: sans-serif;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "  }\n",
    "  \n",
    "  #container {\n",
    "    justify-content: center\n",
    "  }\n",
    "  \n",
    "  h1 {\n",
    "    text-align: center;\n",
    "  }\n",
    "  \n",
    "  p {\n",
    "    margin-bottom: 10px;\n",
    "  }\n",
    "  \n",
    "  input {\n",
    "    width: 100%;\n",
    "    height: 20px;\n",
    "    border: 1px solid black;\n",
    "    margin-bottom: 10px;\n",
    "  }\n",
    "  \n",
    "  button {\n",
    "    height: 20px;\n",
    "    background-color: #000;\n",
    "    color: #fff;\n",
    "    border: none;\n",
    "    cursor: pointer;\n",
    "  }\n",
    "  \n",
    "  #story-output {\n",
    "    width: 100%;\n",
    "    overflow: auto;\n",
    "  }\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524a8cc",
   "metadata": {},
   "source": [
    "### 打包 Lambda 函數為 ZIP 檔案\n",
    "\n",
    "這段程式碼的目的是將 `main.py`、`run.sh` 以及 `static` 目錄打包成一個 `lambda_package.zip` 檔案，方便在 Lambda 上部署。\n",
    "1. **創建臨時目錄**：\n",
    "   - 使用 `os.mkdir` 建立一個臨時目錄 `lambda_temp`，用來存放待打包的檔案。如果目錄已存在，會跳過該步驟。\n",
    "\n",
    "2. **複製檔案**：\n",
    "   - 使用 `shutil.copy` 將 `main.py` 和 `run.sh` 複製到臨時目錄。\n",
    "   - 使用 `shutil.copytree` 將 `static` 資料夾（包含靜態檔案）複製到臨時目錄中。\n",
    "\n",
    "3. **建立 ZIP 檔案**：\n",
    "   - 使用 `zipfile.ZipFile` 將臨時目錄中的檔案壓縮為 `lambda_package.zip`，保留目錄結構，使得 Lambda 可以正確找到這些檔案。\n",
    "\n",
    "4. **清理臨時目錄**：\n",
    "   - 打包完成後，使用 `shutil.rmtree` 刪除臨時目錄，保持工作空間整潔。\n",
    "\n",
    "最後，這段程式碼會在當前工作目錄中生成一個 `lambda_package.zip` 檔案，方便後續上傳至 Lambda 進行部署\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b6b15-6945-487b-b297-6a7c2c5a4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Define the names of your files and directories\n",
    "main_script = 'main.py'\n",
    "run_script = 'run.sh'\n",
    "static_dir = 'static'\n",
    "zip_filename = 'lambda_package.zip'\n",
    "\n",
    "# Step 1: Create a temporary directory to store the files\n",
    "if not os.path.exists('lambda_temp'):\n",
    "    os.mkdir('lambda_temp')\n",
    "\n",
    "# Step 2: Copy your scripts (main.py and run.sh) into the temp directory\n",
    "shutil.copy(main_script, './lambda_temp/')\n",
    "shutil.copy(run_script, './lambda_temp/')\n",
    "\n",
    "# Step 3: Copy the 'static' directory into the temp directory\n",
    "if os.path.exists(static_dir):\n",
    "    shutil.copytree(static_dir, './lambda_temp/static')\n",
    "\n",
    "# Step 4: Create the .zip package\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk('lambda_temp'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, arcname=os.path.relpath(file_path, 'lambda_temp'))\n",
    "\n",
    "# Step 5: Clean up the temp directory\n",
    "shutil.rmtree('lambda_temp')\n",
    "\n",
    "print(f\"{zip_filename} created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5b3b7",
   "metadata": {},
   "source": [
    "# 建立與上傳 FastAPI 相關的 Lambda Layer\n",
    "\n",
    "這段程式碼用於打包並上傳一個 [Lambda Layer](https://docs.aws.amazon.com/zh_tw/lambda/latest/dg/chapter-layers.html)，該 Layer 包含 FastAPI 及其相關的依賴項，方便 Lambda 使用 FastAPI 框架來處理 HTTP 請求。\n",
    "\n",
    "<img src=\"../imgs/lambda-layer.png\" alt=\"Lambda Layer\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2e057-c4de-4a81-8368-b7be07a39ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import subprocess\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# 定義變數\n",
    "layer_dir = \"./layer/python\"\n",
    "zip_filename = \"layer.zip\"\n",
    "region = sess.boto_region_name\n",
    "layer_name = \"fast_api_related_lambda_layer\"\n",
    "layer_description = \"Lambda layer for FastAPI with Lambda Web Adapter\"\n",
    "\n",
    "# Step 1: 建立 layer/python 資料夾\n",
    "if not os.path.exists(layer_dir):\n",
    "    os.makedirs(layer_dir)\n",
    "\n",
    "# Step 2: 安裝指定的套件到 layer/python 資料夾\n",
    "subprocess.run([\n",
    "    \"pip3\", \"install\", \"--target\", layer_dir, \n",
    "    \"annotated-types==0.6.0\", \n",
    "    \"anyio==4.2.0\", \n",
    "    \"click==8.1.7\", \n",
    "    \"exceptiongroup==1.2.0\", \n",
    "    \"fastapi==0.109.2\", \n",
    "    \"h11==0.14.0\", \n",
    "    \"idna==3.7\", \n",
    "    \"pydantic==2.6.1\", \n",
    "    \"pydantic_core==2.16.2\", \n",
    "    \"sniffio==1.3.0\", \n",
    "    \"starlette==0.36.3\", \n",
    "    \"typing_extensions==4.9.0\", \n",
    "    \"uvicorn==0.27.0.post1\"\n",
    "], check=True)\n",
    "\n",
    "# Step 3: 將 layer 目錄壓縮成 .zip 檔案\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk('./layer'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, arcname=os.path.relpath(file_path, './layer'))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{zip_filename} created successfully.\")\n",
    "\n",
    "\n",
    "# 上傳成 Lambda Layer\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "with open(zip_filename, 'rb') as f:\n",
    "    response = lambda_client.publish_layer_version(\n",
    "        LayerName=layer_name,\n",
    "        Description=layer_description,\n",
    "        Content={'ZipFile': f.read()},\n",
    "        CompatibleRuntimes=['python3.11'],  # 根據你的 Lambda 版本設定\n",
    "        LicenseInfo='MIT'  # 可選擇性設定 license\n",
    "    )\n",
    "\n",
    "fastapi_related_layer_arn = response[\"LayerVersionArn\"]\n",
    "\n",
    "# 顯示 Layer ARN\n",
    "print(\"Layer uploaded successfully:\")\n",
    "print(fastapi_related_layer_arn)\n",
    "\n",
    "# 清理 layer 資料夾\n",
    "shutil.rmtree('./layer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa8631",
   "metadata": {},
   "source": [
    "### 定義 CreateStreamingResponseLambdaFunction Step\n",
    "\n",
    "我們會使用到 SageMaker 提供的 [Lambda Hepler](https://sagemaker.readthedocs.io/en/stable/api/utility/lambda_helper.html) 類別來創建 Lambda Function\n",
    "之後使用 [LambdaStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.lambda_step.LambdaStep) 類別來將 Lambda Function 加入到 Pipeline 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f98be-9d63-4a3a-94d4-db21f737737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "from sagemaker.workflow.functions import JsonGet, Join # https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.functions.JsonGet\n",
    "\n",
    "\n",
    "streaming_response_function_name_str=\"lambda_streaming_response_func\"\n",
    "\n",
    "\n",
    "lambda_web_adpter_layer_arn = f\"arn:aws:lambda:{sess.boto_region_name}:753240598075:layer:LambdaAdapterLayerX86:23\"\n",
    "pydantic_layer_arn = \"arn:aws:lambda:us-west-2:770693421928:layer:Klayers-p311-pydantic:10\"\n",
    "\n",
    "\n",
    "# 創建 Lambda 函數物件\n",
    "lambda_function = Lambda(\n",
    "    function_name=streaming_response_function_name_str,\n",
    "    execution_role_arn=lambda_role,\n",
    "    zipped_code_dir=\"lambda_package.zip\",\n",
    "    handler=\"run.sh\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    "    runtime=\"python3.11\",\n",
    "    environment={\n",
    "        \"Variables\": {  \n",
    "            \"SAGEMAKER_ENDPOINT_NAME\": endpoint_name,\n",
    "            \"AWS_LAMBDA_EXEC_WRAPPER\": \"/opt/bootstrap\",\n",
    "            \"AWS_LWA_INVOKE_MODE\": \"response_stream\",\n",
    "            \"PORT\": \"8000\"\n",
    "        }\n",
    "    },\n",
    "    layers=[lambda_web_adpter_layer_arn, pydantic_layer_arn, fastapi_related_layer_arn]\n",
    ")\n",
    "\n",
    "# 創建 Lambda Step\n",
    "lambda_create_streaming_response_step = LambdaStep(\n",
    "    name=\"CreateStreamingResponseLambdaFunction\",\n",
    "    lambda_func=lambda_function,\n",
    "    depends_on=[lambda_deploy_step]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d730-af00-42d5-b74b-ec1a3c960625",
   "metadata": {},
   "source": [
    "## CreateLambdaFunctionURL Step\n",
    "\n",
    "在這個步驟中，我們將為之前創建的 Streaming Response Lambda Function 生成一個 [Lambda Function URL](https://docs.aws.amazon.com/lambda/latest/dg/urls-configuration.html)，這樣外部客戶端可以通過這個 URL 訪問 Lambda，並發送推理請求。\n",
    "\n",
    "### 為什麼選擇 Lambda Function URL？\n",
    "- 主要原因是 [Lambda Function URL 支援 Streaming Response](https://docs.aws.amazon.com/lambda/latest/dg/response-streaming-tutorial.html)。\n",
    "- 使用 Lambda Function URL 可以簡化用戶端與 Lambda 之間的互動，不需要經過 API Gateway 的額外配置與流量控制。這對於快速實現 Lambda 與用戶端的直接通信非常有幫助。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9e4c6-e982-4635-a52d-f7d758b76344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_create_function_url.py\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    lambda_client = boto3.client('lambda')\n",
    "    function_name = event['function_name']\n",
    "    \n",
    "    # 創建 Lambda Function URL\n",
    "    response = lambda_client.create_function_url_config(\n",
    "        FunctionName=function_name,\n",
    "        AuthType='NONE',\n",
    "        InvokeMode='RESPONSE_STREAM'\n",
    "    )\n",
    "    \n",
    "    # 返回 Function URL\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': {\n",
    "            'function_url': response['FunctionUrl']\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cb7ba-a2d6-422c-aaf4-dc1bcd8ec3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "\n",
    "\n",
    "# 創建 Lambda 函數，用於創建 Function URL\n",
    "lambda_create_function_url = Lambda(\n",
    "    function_name=\"create_lambda_function_url\",\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"lambda_create_function_url.py\",\n",
    "    handler=\"lambda_create_function_url.lambda_handler\",\n",
    "    timeout=300,\n",
    "    memory_size=128\n",
    ")\n",
    "\n",
    "# 創建 LambdaStep\n",
    "create_lambda_function_url_step = LambdaStep(\n",
    "    name=\"CreateLambdaFunctionURL\",\n",
    "    lambda_func=lambda_create_function_url,\n",
    "    inputs={\n",
    "        \"function_name\": streaming_response_function_name_str\n",
    "    },\n",
    "    outputs=[\n",
    "        LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String),\n",
    "        LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "    ],\n",
    "    depends_on=[lambda_create_streaming_response_step]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e75618-de5a-4f05-9619-c48e06645c80",
   "metadata": {},
   "source": [
    "## 定義與創建 SageMaker Pipeline\n",
    "\n",
    "這段程式碼負責定義整個 SageMaker Pipeline，並將之前設定的所有步驟串聯起來，最後將其創建或更新到 SageMaker 中。\n",
    "\n",
    "### 主要步驟：\n",
    "1. **Pipeline 定義**:\n",
    "   - `Pipeline` 物件中包含 Pipeline 的名稱、參數、以及所有的步驟。\n",
    "   - `parameters` 定義了我們的 Pipeline 會使用哪些可變的輸入參數，例如訓練數據集的 S3 URI 和推理實例類型。這些參數讓 Pipeline 更加靈活，可以根據不同的需求進行調整。\n",
    "   - `steps` 列表包含了前面定義的所有步驟，依次為：\n",
    "     - **train_step**: 模型訓練步驟，fine-tune 預訓練模型。\n",
    "     - **register_step**: 模型註冊步驟，將訓練完成的模型註冊到 SageMaker Model Registry 中。\n",
    "     - **lambda_deploy_step**: Lambda 部署步驟，將模型部署到 SageMaker Endpoint。\n",
    "     - **lambda_create_streaming_response_step**: 創建 Streaming Response Lambda，處理推理請求並返回流式回應。\n",
    "     - **create_lambda_function_url_step**: 創建 Lambda Function URL，公開 Lambda 讓外部系統可以訪問推理服務。\n",
    "\n",
    "2. **Pipeline 更新或創建**:\n",
    "   - 使用 `pipeline.upsert(role_arn=role)` 將 Pipeline 創建或更新到 SageMaker 中。如果這個 Pipeline 已經存在，則會更新；如果不存在，則會創建一個新的。\n",
    "   - `role_arn` 參數指定了 SageMaker Pipeline 執行所需的 IAM 角色，該角色擁有執行 Pipeline 各步驟的必要權限。\n",
    "\n",
    "當你執行下方 Code Cell 之後，我們可以到 **SageMaker Studio >  Pipelines** 找到我們所創建的 Pipeline\n",
    "\n",
    "<img src=\"../imgs/sagemaker-piepline-list.png\" alt=\"SageMaker Pipeline List\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0a9ec-6593-42d2-976d-bf2f4b9847dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"Parameterize-UncompressedModelArtifact-Deploy-ExposeEndpoint-Pipeline\",\n",
    "    parameters=[inference_instance_type, model_artifact_s3_uri, model_name, random_string],\n",
    "    steps=[register_step, lambda_deploy_step, lambda_create_streaming_response_step, create_lambda_function_url_step]\n",
    ")\n",
    "\n",
    "# 更新 SageMaker Pipeline (不存在則創建)\n",
    "pipeline.upsert(role_arn=role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13dcfc0",
   "metadata": {},
   "source": [
    "## 執行 SageMaker Pipeline\n",
    "\n",
    "在這裡我將示範如何在 SageMaker Studio 的頁面中執行我們創建的 Pipeline\n",
    "\n",
    "打開 SageMaker Studio > Pipelines，點擊要執行的 Pipeline\n",
    "\n",
    "<img src=\"../imgs/sagemaker-piepline-list.png\" alt=\"SageMaker Pipeline List\" width=\"400\"/>\n",
    "\n",
    "點擊右上角 **Execute**\n",
    "\n",
    "- **Execution name:** 20240912-0128\n",
    "- **Description - optional:** Demo pipeline, Train, Register, Deploy\n",
    "- **TrainingDatasetesS3Uri (String): <**提供你的 S3 prefix**>**\n",
    "    - 例如: 你的 `train_datasets.json` ，放在 `s3://sagemaker/trainingdata/` 那這欄位就是填寫 \"`s3://sagemaker/trainingdata/`\"\n",
    "- **InferenceInstanceType (String)**: 設定推理所需的實例類型，默認值為 `\"ml.g5.xlarge\"`\n",
    "\n",
    "<img src=\"../imgs/sagemaer-pipeline-execute.png\" alt=\"SageMaker Pipeline Execute\" width=\"400\"/>\n",
    "\n",
    "\n",
    "## 查看 Execution 結果\n",
    "\n",
    "當我們執行 Pipeline 之後，可以點擊 Executions 頁籤來查看執行的結果\n",
    "\n",
    "<img src=\"../imgs/sagemaker-pipeline-execution-list.png\" alt=\"sagemaker-pipeline-execution-list\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "我們可以用滑鼠雙擊特定 Step 來查看詳細結果以及 Output，以我下圖為例，我想要找到具體 Function URL ，因此我雙擊 `CreateLambdaFunctionURL` Step，並且從 Output 中找到 `Body`，這是 Lambda Function Return 的 Json Object，裡面包含了我們的 Function URL\n",
    "\n",
    "<img src=\"../imgs/sagemaker-pipeline-step-execution-details.png\" alt=\"sagemaker-pipeline-step-execution-details\" width=\"400\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
