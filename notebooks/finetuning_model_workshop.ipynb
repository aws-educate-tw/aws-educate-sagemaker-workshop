{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Makrdown cellé»é–‹ä¾†æœ‰æ¨™å¥½TODOè¦å¹«å¿™ä¿®æ”¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy open LLMs with Amazon SageMaker\n",
    "## TODO: \n",
    "### 1. Modelè¦æ›æˆphi-3, è¦è«‹yunaæ¸¬è©¦ä¸€ä¸‹å¹¾ç­†è³‡æ–™å¯ä»¥è·‘å®Œæ•´å€‹notebook, ä¸åŒ…å«deploy\n",
    "### 2. Deployé€™é‚Šè¦è«‹Shiunä¾†ä¿®æ”¹\n",
    "### 3. é€™æ¬¡ä¸æœƒè®“ä»–å€‘å¾hfæ“ä½œä»»ä½•æ±è¥¿, ä¸ç®¡æ˜¯download modelé‚„æ˜¯dataset,å¯ä»¥æä¸€ä¸‹å¦‚æœå°å…¶ä»–Modelæˆ–æ˜¯Datasetæœ‰èˆˆè¶£å¯ä»¥æ€éº¼æ“ä½œ \n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune open LLMs, like [Mistral](https://huggingface.co/models?other=mistral) using [QLoRA](https://arxiv.org/abs/2305.14314) and how to deploy them afterwards using the <!-- TODO: -->  [Hugging Face LLM Inference DLC](https://huggingface.co/blog/sagemaker-huggingface-llm) @shiun\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). We will also make use of new and efficient features and methods including, Flash Attention, Datset Packing and Mixed Precision Training. @richie\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **è¨­ç½®é–‹ç™¼ç’°å¢ƒ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> è‹¥æƒ³åœ¨è‡ªå·±çš„ Hugging Face ä¸‹è¼‰æ¨¡å‹ï¼Œè«‹ç™»å…¥è‡ªå·±çš„å¸³è™Ÿã€ç”¢ç”Ÿ tokenã€åŒæ„æ¨¡å‹ä½¿ç”¨æ¢æ¬¾ã€ä¸¦ä¿®æ”¹ç¨‹å¼ç¢¼ã€‚   \n",
    "> <font color=orange>**ç”±æ–¼æ™‚é–“å› ç´ ï¼Œæœ¬æ¬¡å·¥ä½œåŠå·²å°‡æ¨¡å‹æ”¾ä¸Š S3ï¼Œä¸æœƒå¸¶å¤§å®¶ä¸‹è¼‰æ¨¡å‹**</font>  \n",
    "> \n",
    "> è‹¥æƒ³åœ¨åœ°ç«¯ç’°å¢ƒä½¿ç”¨ SageMakerï¼Œéœ€è¦æ“æœ‰å…·å‚™ SageMaker æ‰€éœ€æ¬Šé™çš„ IAM roleï¼Œæ›´å¤šè³‡è¨Šè«‹åƒè€ƒ [\\[How to use SageMaker execution roles\\]](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **å®‰è£ä½¿ç”¨ Hugging Face æ‰€éœ€å¥—ä»¶**\n",
    "\n",
    "1. `huggingface_hub`: ç”¨æ–¼èˆ‡ Hugging Face æ¨¡å‹å’Œè³‡æ–™åº«äº’å‹•ï¼Œå…è¨±ç”¨æˆ¶ä¸Šå‚³å’Œä¸‹è¼‰é è¨“ç·´çš„æ¨¡å‹ã€å…±äº«å’Œç®¡ç†æ¨¡å‹ã€‚\n",
    "   \n",
    "2. `transformers`: ç”¨æ–¼è‡ªç„¶èªè¨€è™•ç† (NLP) ä»»å‹™ï¼Œæ”¯æ´å¤šç¨®é è¨“ç·´çš„æ¨¡å‹æ¶æ§‹ï¼Œä¸¦æä¾›äº†åŠ è¼‰ã€å¾®èª¿ã€å’Œæ‡‰ç”¨æ¨¡å‹çš„å·¥å…·ã€‚\n",
    "   \n",
    "3.  `--quiet`: è®“å®‰è£éç¨‹ä¸­çš„è¼¸å‡ºä»‹é¢ä¿æŒæ•´æ½”ï¼Œä¸æœƒå°å‡ºéå¤šçš„å®‰è£ç´°ç¯€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub==0.24.6 transformers==4.44.2 datasets==2.20.0 --no-deps --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **è¨­å®š SageMaker ç’°å¢ƒä¸¦ç²å–ç›¸é—œçš„ AWS IAM è§’è‰²å’Œ S3 bucket è³‡è¨Š**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Creates a SageMaker session to manage operations related to SageMaker\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# Setup SageMaker storage bucket:\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# Get execution role\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Recreates the SageMaker session using the previously obtained sagemaker_session_bucket\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **è™•ç†è³‡æ–™é›†**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘å€‘æœƒç”¨æˆ‘å€‘é å…ˆæº–å‚™çš„datasetä¾†è™•ç†, é€™æ¬¡datasetæ˜¯é€éaiç”¢çš„, æˆ‘å€‘å¯«äº†è…³æœ¬å¯ä»¥å»call bedrock apiå’Œclaude 3 ä¾†å¹«æˆ‘å€‘ç”¢è³‡æ–™, è…³æœ¬å¯ä»¥çœ‹ [generate_data.py](./scripts/generate_data.py) å’Œ [generate_data_2.py](./scripts/generate_data_2.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **å°å…¥é è¨“ç·´çš„ tokenizer (åˆ†è©å™¨)**\n",
    "\n",
    "- `AutoTokenizer` å¯ä»¥è‡ªå‹•æ ¹æ“šæä¾›çš„æ¨¡å‹ ID é¸æ“‡é©åˆçš„ tokenizer é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-3.5-mini-instruct\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **å°‡æœ¬æ¬¡ workshop æä¾›çš„ dataset è¼‰å…¥ SageMaker bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=\"us-west-2\")\n",
    "\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def copy_s3_object(source_uri, target_bucket):\n",
    "    source_bucket, source_key = parse_s3_uri(source_uri)\n",
    "    try:\n",
    "        # Download file from source bucket\n",
    "        response = s3_client.get_object(Bucket=source_bucket, Key=source_key)\n",
    "        file_content = response['Body'].read()\n",
    "\n",
    "        # Upload file to target bucket\n",
    "        s3_client.put_object(Bucket=target_bucket,\n",
    "                             Key=source_key, Body=file_content)\n",
    "        print(f\"Successfully copied {source_key} to {target_bucket}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {source_key}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Base S3 URI for datasets\n",
    "base_uri = 's3://aws-educate-09-28-sagemaker-workshop-oregon/datasets/phi-3.5-mini-instruct/workshop/'\n",
    "train_uri = base_uri + 'train_dataset.json'\n",
    "test_uri = base_uri + 'test_dataset.json'\n",
    "\n",
    "# Define your target S3 bucket\n",
    "target_bucket = sagemaker_session_bucket\n",
    "\n",
    "# Copy train and test datasets to the new S3 bucket\n",
    "copy_s3_object(train_uri, target_bucket)\n",
    "copy_s3_object(test_uri, target_bucket)\n",
    "\n",
    "# Construct the S3 URI for the datasets in the target bucket\n",
    "datasets_s3_uri = f\"s3://{target_bucket}/datasets/phi-3.5-mini-instruct/workshop/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **å¾ Amazon S3 è®€å– JSON æ ¼å¼çš„ datasetï¼Œä¸¦å°‡å…¶æ ¼å¼åŒ–ç‚ºå¯ä»¥ä½¿ç”¨çš„ dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import boto3\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def read_and_format_json_from_s3(s3_uri):\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        file_content = obj['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # parse json\n",
    "        try:\n",
    "            data = json.loads(file_content)\n",
    "        except json.JSONDecodeError:\n",
    "            data = []\n",
    "            for line in file_content.splitlines():\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    data.append(item)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        formatted_data = []\n",
    "        for item in data:\n",
    "            formatted_item = {}\n",
    "            if 'messages' in item:\n",
    "                formatted_item['input'] = item['messages'][0]['content']\n",
    "                formatted_item['output'] = item['messages'][1]['content']\n",
    "            else:\n",
    "                formatted_item = item\n",
    "            formatted_data.append(formatted_item)\n",
    "\n",
    "        return formatted_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from S3: {e}\")\n",
    "        return None\n",
    "    \n",
    "# s3 URI\n",
    "base_uri = 's3://aws-educate-09-28-sagemaker-workshop-oregon/datasets/phi-3.5-mini-instruct/workshop/'\n",
    "train_uri = base_uri + 'train_dataset.json'\n",
    "test_uri = base_uri + 'test_dataset.json'\n",
    "\n",
    "# Load and format training and test data from S3\n",
    "train_data = read_and_format_json_from_s3(train_uri)\n",
    "test_data = read_and_format_json_from_s3(test_uri)\n",
    "\n",
    "# Create DatasetDict from the formatted data\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(train_data)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "})\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)\n",
    "\n",
    "# Dataset Features\n",
    "print(\"\\n Dataset Features:\")\n",
    "print(dataset['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **å°‡è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™ä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶è·¯å¾‘ä¸­**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the directory to save the datasets\n",
    "save_path = '/opt/ml/input/data/training'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_dataset(file_path, data):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "# Save training data\n",
    "train_file_path = os.path.join(save_path, 'train_dataset.json')\n",
    "save_dataset(train_file_path, train_data)\n",
    "\n",
    "# Save test data\n",
    "test_file_path = os.path.join(save_path, 'test_dataset.json')\n",
    "save_dataset(test_file_path, test_data)\n",
    "\n",
    "print(f\"Dataset saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **å˜—è©¦è®€å– S3 ä¸­çš„ dataset (output.json) ä¸¦é¡¯ç¤ºçµæœ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def read_and_display_s3_data(s3_uri, limit=3):\n",
    "    # Parse the S3 URI into bucket and key\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "\n",
    "    try:\n",
    "        # Retrieve the object from S3\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        \n",
    "        # Read and decode the data\n",
    "        data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        # Parse the JSON data\n",
    "        json_data = json.loads(data)\n",
    "        \n",
    "        # Display a limited number of conversations\n",
    "        for idx, item in enumerate(json_data[:limit], 1):\n",
    "            print(f\"\\nConversation {idx}:\")\n",
    "            for message in item.get('messages', []):\n",
    "                role = message.get('role', 'unknown').capitalize()\n",
    "                content = message.get('content', 'No content')\n",
    "                print(f\"  {role}: {content}\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error reading S3 data: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON data: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "output_uri = base_uri + 'output.json'\n",
    "read_and_display_s3_data(output_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **é€²è¡Œ Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨é€™å€‹éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡æ·±å…¥æ¢è¨ç¥ç¶“ç¶²çµ¡çš„ä¸€äº›åŸºæœ¬åŸç†ï¼Œä»¥åŠå¦‚ä½•é€šéèª¿æ•´è¶…åƒæ•¸ä¾†å„ªåŒ–æ¨¡å‹è¨“ç·´ã€‚æˆ‘ä¹Ÿæœƒä»‹ç´¹ QLoRAï¼ˆQuantization-aware Low-Rank Adaptationï¼‰çš„åŸºæœ¬åŸç†ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œè®“æˆ‘å€‘å›é¡§ä¸€äº›é—œéµæ¦‚å¿µï¼š\n",
    "\n",
    "\n",
    "#### ç¥ç¶“ç¶²çµ¡åŸºç¤æ¦‚å¿µ(Neural Network Fundamentals)\n",
    "\n",
    "1. **ç¥ç¶“ç¶²çµ¡**\n",
    "   - æ¨¡ä»¿äººè…¦çµæ§‹çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹\n",
    "   - ç”±å¤šå±¤ç¥ç¶“å…ƒçµ„æˆï¼Œé€šéæ¬Šé‡(weight)å’Œæ¿€æ´»å‡½æ•¸(activation function)è™•ç†è¼¸å…¥æ•¸æ“š\n",
    "   - å»£æ³›æ‡‰ç”¨æ–¼åœ–åƒè­˜åˆ¥(image recognition)ã€è‡ªç„¶èªè¨€è™•ç†(NLP)ã€èªéŸ³è­˜åˆ¥(speech recognition)ç­‰é ˜åŸŸ\n",
    "\n",
    "2. **å‰å‘å‚³æ’­ (Forward Propagation)**\n",
    "   - æ•¸æ“šå¾è¼¸å…¥å±¤(input layer)é€šééš±è—å±¤(hidden layer)åˆ°è¼¸å‡ºå±¤(output layer)çš„éç¨‹\n",
    "   - æ¯ä¸€å±¤çš„è¼¸å‡º(output)ä½œç‚ºä¸‹ä¸€å±¤çš„è¼¸å…¥(input)\n",
    "\n",
    "3. **åå‘å‚³æ’­ (Backward Propagation)**\n",
    "   - è¨ˆç®—æå¤±å‡½æ•¸(loss function)å°æ¯å€‹æ¬Šé‡(weight)çš„æ¢¯åº¦(gradient)\n",
    "   - å¾è¼¸å‡ºå±¤(output layer)å‘è¼¸å…¥å±¤(input layer)é€å±¤èª¿æ•´æ¬Šé‡(weight)\n",
    "\n",
    "4. **æ¢¯åº¦ä¸‹é™ (Gradient Descent)**\n",
    "   - å„ªåŒ–ç¥ç¶“ç¶²çµ¡çš„æ ¸å¿ƒç®—æ³•\n",
    "   - é€šéæ²¿æ¢¯åº¦(gradient)çš„åæ–¹å‘(negative direction)èª¿æ•´æ¬Šé‡(weight)ä¾†æœ€å°åŒ–æå¤±å‡½æ•¸(loss function)\n",
    "\n",
    "#### é—œéµè¶…åƒæ•¸(Hyperparameter)\n",
    "\n",
    "5. **æ‰¹é‡å¤§å° (Batch Size)**\n",
    "   - æ¯æ¬¡æ›´æ–°æ¬Šé‡æ™‚ä½¿ç”¨çš„è¨“ç·´æ¨£æœ¬æ•¸\n",
    "   - è¼ƒå¤§çš„æ‰¹é‡å¯ä»¥æé«˜è¨“ç·´ç©©å®šæ€§ï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šå…§å­˜\n",
    "\n",
    "6. **å­¸ç¿’ç‡ (Learning Rate)**\n",
    "   - æ§åˆ¶æ¯æ¬¡è¿­ä»£(iteration)æ™‚æ¬Šé‡èª¿æ•´çš„å¹…åº¦\n",
    "   - å¤ªå¤§å¯èƒ½å°è‡´ä¸æ”¶æ–‚ï¼Œå¤ªå°å¯èƒ½å°è‡´è¨“ç·´éæ…¢\n",
    "\n",
    "#### é«˜ç´šæŠ€è¡“(Advanced Techniques)\n",
    "\n",
    "7. **é‡åŒ– (Quantization)**\n",
    "   - å°‡æ¨¡å‹åƒæ•¸å¾é«˜ç²¾åº¦è½‰æ›ç‚ºä½ç²¾åº¦\n",
    "   - å¯ä»¥æ¸›å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ™‚é–“ï¼Œä½†å¯èƒ½ç•¥å¾®é™ä½ç²¾åº¦\n",
    "\n",
    "8. **LoRA (Low-Rank Adaptation)**\n",
    "   - ä¸€ç¨®é«˜æ•ˆçš„æ¨¡å‹å¾®èª¿æŠ€è¡“\n",
    "   - ä¸»è¦åƒæ•¸ï¼š\n",
    "     - alpha: æ§åˆ¶LoRAæ›´æ–°çš„å¼·åº¦\n",
    "     - rank: æ±ºå®šé©é…å™¨çŸ©é™£çš„ç§©ï¼Œå½±éŸ¿æ¨¡å‹è¡¨é”èƒ½åŠ›å’Œè¨ˆç®—æˆæœ¬\n",
    "\n",
    "   - ç·šæ€§ä»£æ•¸ä¸­çš„ rank æ¦‚å¿µï¼š\n",
    "     - åœ¨å¤§å‹ç¥ç¶“ç¶²çµ¡ä¸­ï¼Œæ¬Šé‡æ›´æ–°é€šå¸¸æ¶‰åŠé«˜ç¶­çŸ©é™£\n",
    "     - LoRA å‡è¨­é€™äº›æ›´æ–°å¯ä»¥ç”¨ä½ç§©çŸ©é™£(low rank)ä¾†è¿‘ä¼¼ï¼Œå¾è€Œæ¸›å°‘è¨“ç·´åƒæ•¸æ•¸é‡\n",
    "     - é€šéèª¿æ•´ rank åƒæ•¸ï¼Œå¯ä»¥åœ¨æ¨¡å‹è¤‡é›œåº¦å’Œè¨ˆç®—æ•ˆç‡ä¹‹é–“å–å¾—å¹³è¡¡\n",
    "\n",
    "9. **QLoRA (Quantization-aware Low-Rank Adaptation)**\n",
    "   - çµåˆé‡åŒ–å’Œ LoRA çš„æŠ€è¡“\n",
    "   - å„ªåŒ–æ­¥é©Ÿï¼š\n",
    "     1. å°‡é è¨“ç·´æ¨¡å‹é‡åŒ–å¾Œä¸¦å‡çµ\n",
    "     2. æ·»åŠ å°å‹ã€å¯è¨“ç·´çš„ LoRA é©é…å™¨å±¤\n",
    "     3. åƒ…å¾®èª¿é©é…å™¨å±¤ï¼ŒåŒæ™‚ä½¿ç”¨å‡çµçš„é‡åŒ–æ¨¡å‹ä½œç‚ºä¸Šä¸‹æ–‡\n",
    "   - å„ªé»ï¼šå¤§å¤§æ¸›å°‘å…§å­˜éœ€æ±‚ï¼ŒåŒæ™‚ä¿æŒæ¨¡å‹æ€§èƒ½\n",
    "\n",
    "é€™äº›è¶…åƒæ•¸å’ŒæŠ€è¡“åœ¨è¨“ç·´è…³æœ¬ [run_qlora.py](../scripts/run_qlora.py) ä¸­è¢«ä½¿ç”¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 3,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 2,                 # Number of updates steps to accumulate \n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'fp16': True ,\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"constant\",                   # learning rate scheduler\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run',                         # output directory, where to save assets during training\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token() # huggingface token to access gated models, e.g. llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **è¨­ç½®è¨“ç·´ä»»å‹™**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora.py',    # train script\n",
    "    source_dir           = '../scripts',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **è¨­å®šè¨“ç·´æ•¸æ“šçš„ä½ç½®ï¼Œä¸¦å•Ÿå‹• Hugging Face æ¨¡å‹çš„è¨“ç·´ä»»å‹™**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a data input dictionary with the uploaded S3 URIs\n",
    "data = {\n",
    "    'training': f\"s3://{sess.default_bucket()}/datasets/phi-3.5-mini-instruct/\"\n",
    "}\n",
    "\n",
    "# Start the training job using the provided datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **å°‡ S3 çš„ URI è½‰æ›ç‚ºä¸€å€‹å¯ä»¥ç›´æ¥åœ¨ AWS S3 ç®¡ç†æ§åˆ¶å°ä¸­è¨ªå•çš„ URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"].replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ğŸš€<font color=orange>**Now, lets deploy our model to an endpoint. ğŸš€**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "## <font color=red>**ä»¥ä¸‹ç‚ºå­˜æª”**</font>\n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    ".   \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune Mistral 7B with QLoRA on Amazon SageMaker\n",
    "## TODO\n",
    "### ä¸€æ¨£modelè¦æ”¹, é€™æ¬¡è¨“ç·´è…³æœ¬ç¢ºå¯¦æ˜¯ç”¨run_qlora\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2305.14314)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_qlora.py](./scripts/run_qlora.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In Addition to QLoRA we will leverage the new [Flash Attention 2 integrationg with Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flash-attention-2) to speed up the training. Flash Attention 2 is a new efficient attention mechanism that is up to 3x faster than the standard attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 3,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 2,                 # Number of updates steps to accumulate \n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'fp16': True ,\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"constant\",                   # learning rate scheduler\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run',                         # output directory, where to save assets during training\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token() # huggingface token to access gated models, e.g. llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. Amazon SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "> Note: Make sure that you include the `requirements.txt` in the `source_dir` if you are using a custom training script. We recommend to just clone the whole repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora.py',    # train script\n",
    "    source_dir           = '../scripts',      # directory which includes all the files needed for training\n",
    "    #TODO: é€™é‚Šå¯ä»¥ç¢ºå®šä¸€ä¸‹é‚„æœ‰å“ªäº›instaceå¯ä»¥ç”¨ https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for Mistral 7B, the SageMaker training job took `13968 seconds`, which is about `3.9 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned Mistral model was only ~`$8`. \n",
    "\n",
    "Now lets make sure SageMaker has successfully uploaded the model to S3. We can use the `model_data` property of the estimator to get the S3 path to the model. Since we used `merge_weights=True` and `disable_output_compression=True` the model is stored as raw files in the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"].replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a similar folder structure and files in your S3 bucket:\n",
    "\n",
    "![S3 Bucket](../assets/s3.png)\n",
    "\n",
    "Now, lets deploy our model to an endpoint. ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## éƒ¨ç½²æ¨¡å‹\n",
    "\n",
    "\n",
    "åœ¨å‰é¢è¨“ç·´å¥½æ¨¡å‹ä¹‹å¾Œï¼Œæˆ‘å€‘å¯ä»¥å¾ **SageMaker > Training > Training Job** è£¡é¢æ‰¾åˆ° Model çš„ S3 è·¯å¾‘ï¼š\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  # version=\"1.1.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘å€‘ç¾åœ¨å¯ä»¥ä½¿ç”¨å®¹å™¨ URI å’Œæ¨¡å‹åœ¨ S3 çš„è·¯å¾‘ä¾†å‰µå»ºä¸€å€‹ `HuggingFaceModel`ã€‚åŒæ™‚ï¼Œæˆ‘å€‘é‚„éœ€è¦è¨­å®š TGIï¼ˆText Generation Inferenceï¼‰çš„é…ç½®ï¼ŒåŒ…æ‹¬ GPU çš„æ•¸é‡å’Œæœ€å¤§è¼¸å…¥ tokensã€‚ä½ å¯ä»¥åœ¨[é€™è£¡](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher)æ‰¾åˆ°å®Œæ•´çš„é…ç½®é¸é …åˆ—è¡¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the HuggingFaceModel we can deploy it to Amazon SageMaker using the deploy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Deploy from Model Registry\n",
    "\n",
    "ä¸‹æ–¹ç‚ºç¤ºç¯„å¦‚ä½•å¾ Model Hub éƒ¨ç½²æ¨¡å‹ï¼Œè‹¥æœ‰éœ€è¦å¯ä»¥åƒè€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.ModelPackage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_package_arn = \"arn:aws:sagemaker:us-west-2:097724924093:model-package/Demo-SageMaker-Pipeline-Group/3\"\n",
    "model = ModelPackage(role=role, # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.ModelPackage\n",
    "                     model_package_arn=model_package_arn,\n",
    "                     sagemaker_session=sess)\n",
    "model.deploy(initial_instance_count=1, instance_type='ml.g5.xlarge', container_startup_health_check_timeout=1000) # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
